---
layout: post
title: 'LongVLM: Efficient Long Video Understanding  via Large Language Models ECCV 2024😊'
subtitle: 'LongVLM: 高效的长视频理解通过大语言模型'
date: 2025-04-27
author: Sun
cover: 'https://pic1.imgdb.cn/item/680c73e158cb8da5c8ce297e.png'
tags: 论文阅读
---

> [LongVLM: Efficient Long Video Understanding  via Large Language Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04936.pdf)

> 💐💐[提供代码](https://github.com/ziplab/LongVLM)
> 
> 📌作者单位
> 
> 1. ZIP Lab, Monash University, Australia
> 2. 中国科学技术大学 信息科学技术学院
> 3. Department of Computer Vision, MBZUAI（穆罕默德·本·扎耶德人工智能大学）
> 4. ReLER, AAII, UTS（悉尼科技大学下属的人工智能研究所中的视觉识别与学习实验室）

# 1.文章针对痛点

这篇文章关注的问题是视频大语言模型。文章认为<mark>现有的基于视频的大语言模型仍然面临一个明显的挑战，就是必须处理大量的*tokens*，</mark>从而联合建模时空跨连续视频帧的依赖关系。这样往往**会造成计算成本很高**。

虽然针对这个问题（需要处理大量的*tokens* ），现有方法通过池化或者查询聚合操作来解决这个问题，但是这种方式文章认为也带来了新的问题，就是<mark>不能很好地细粒度理解长视频信息</mark>。

针对上面的问题，也有研究方法进行了相应的处理，但是文章认为虽然这些方法可以成功地捕捉全局语义信息，但是仍然<mark>忽视了保留短期片段的局部信息以及不同短期片段的时间结构</mark>，例如事件或子事件的顺序。然而，仅通过局部特征序列来建立时间结构模型，仍可能导致不同片段的识别不一致，妨碍对视频的整体理解。

# 2.主要贡献

所以针对上面的内容，文章提出了*LongVLM*，一种简单但是高效的长视频理解模型。

文章<mark>提取视频特征作为短期局部特征序列，并整合全局语义到每个短时片段特征中</mark>。具体来说，首先从**长期视频中均匀采样视频帧序列**，然后**利用预先训练好的视觉编码器**（如 *CLIP-ViT-L/14* ）**提取**每个单独视频帧的**视觉特征**。这些帧级特征包括来自一系列编码器层的 [CLS] 标记和来自视觉编码器最后第二层的*patch* 特征。<mark>然后</mark>，沿时间维度划分*patch* 特征序列，形成多个短时片段。每个片段被视为视频中的一个局部单元，并包含该片段内视频帧的*patch* 特征。**为了降低计算成本并获得每个片段的紧凑特征，文章采用了标记合并模块**，将特定片段的这些*patch* 特征聚合成一个浓缩的标记集。这样，获得了每个片段的局部特征。

之后，按顺序串联这些特征，以明确保留长期视频中短期片段的时间顺序。此外，文章将每个视频帧中的 [CLS] 标记沿时间维度进行平均，以表示整个视频的全局语义信息。**为了整合全局信息**，文章将平均后的[CLS] 标记预置在片段级特征之前，然后通过投影层将其输入 *LLM*。利用 *LLM* 中的因果注意机制，同时实现了对短期片段序列的时间结构建模以及将全局语义信息投影到局部特征中。

原文总结贡献如下：

1. 我们提出了 *LongVLM*，这是一种简单而有效的视频*LLM*，用于在细粒度水平上高效地理解长期视频，同时保持可承受的计算成本。
2. 我们建议将长视频分解成短片段，并提取每个片段的局部特征，以保留其时间顺序。为了紧凑地表示每个片段，我们提出了一个分层标记合并模块来聚合视觉标记。此外，我们还将全局语义整合到每个片段中，以增强对上下文的理解。
3. 在 *VideoChatGPT* 基准和零样本视频问题解答数据集上进行的广泛实验表明，我们的 *LongVLM* 大大超越了之前最先进的方法，同时能为长期视频生成更精确、更准确的细粒度响应。

# 3.实现流程

一样，还是先看一下原文展示的模型架构图：
![模型架构图](https://pic1.imgdb.cn/item/680c793c58cb8da5c8ce2cb7.png)

文章提出的这个架构包含三个部分：一个视觉编码器，一个投影层和一个大语言模型。

整体来说，视觉编码器会编码采样的每一帧，得到帧级别特征$$\left\{\mathbf{X}^{t}, \mathbf{P}^{t}\right\}_{t=1}^{T}$$。之后，为了能够进行细粒度理解，将编码器获取的特征分为了多个短时片段，最后处理这些短时片段，得到全局特征和局部特征。

将全局特征和局部特征序列转入线性层，从而获得投影视觉特征。投影视觉特征与标记化的系统命令和用户查询进行连接，并输入 *LLM* 以生成响应。

# 4.实现细节

* **局部特征聚合**：这个模块是处理视频编码器获得的帧级别特征，文章提到先前的方式是采用因子时空池化或利用查询聚合来处理所有的视觉*tokens*，但是文章认为这可能会丢失与短时事件或动作相关的局部信息；但是不处理的话，会产生由时空冗余造成的冗余计算花费。<mark>因此，文章提出在每个短期片段中聚合紧凑的视觉特征</mark>。具体来说，收集$$s^{th}$$片段的特征$$\mathbf{V}^{s}=\left\{\mathbf{P}^{t}\right\}_{t=(s-1) K}^{s K} \in \mathbb{R}^{K N \times d}$$，之后应用一个分层*token merging* 模块，文章采用的**双向软匹配方法**。具体而言，就是将该短时片段的所有*tokens*，分成了两个完全不相交的子集；之后根据*patch* 特征计算相似性分数，如下公式所示；选择相似性分数前$$r_i$$对，进行池化合并处理；最后，将所有剩下的*tokens* 重新连接在一起；重复上述过程，直至最终*tokens* 数量达到目标，得到$$\mathbf{Z}^{s}=\{\mathbf{z}_{m}\}_{m=1}^{M}\in\mathbb{R}^{M\times d}.$$。最后这些片段级特征按顺序连接为局部特征序列$$\mathbf{L}=\{\mathbf{Z}^s\}_{s=1}^S=[\mathbf{z}_1^1,...,\mathbf{z}_M^1,...,\mathbf{z}_1^S,...,\mathbf{z}_M^S]$$。<mark>这样可以有效地为每个片段编码紧凑的局部特征，同时消除视觉标记序列中的冗余</mark>。
  
  $$
  a^{p_iq_i}=\frac{1}{C}[\sum_{c=1}^Ccos(\mathbf{p}_c^{(p_i)},\mathbf{p}_c^{(q_i)})],	\quad(1)
  $$
* **全局语义整合**：这个模块是考虑到虽然局部特征聚合模块可以提供不同事件或动作的细粒度信息，增强模型的细节理解能力；但每个片段的局部特征不足以帮助模型建模不同片段之间的推理关系，并给出整个视频的合理回答。所以，文章<mark>引入了全局语义特征来丰富具有上下文信息的局部特征</mark>。具体来说，文章将前面*E* 个编码器层得到的[CLS] *tokens* 沿着时间维度平均，得到$$\overline{\mathbf{X}}_{e}=\operatorname{AvgPool}\left(\left\{\mathbf{x}_{e}^{t}\right\}_{t=1}^{T}\right) \in   \mathbb{R}^{d}, e \in[1, \ldots, E]$$，最后依次串联这些平均后得到的特征，得到全局特征$$\mathbf{G}=\left\{\overline{\mathbf{X}}_{e}\right\}_{e=1}^{E} \in \mathbb{R}^{E \times d}$$。
  
  最后就是投影层将这些视觉特征转换到文本特征空间，与指令连接后被送入大语言模型作为输入。

# 5.模型性能

实验性能效果对比图：![效果对比图1](https://pic1.imgdb.cn/item/680d90c858cb8da5c8cfda51.png)
![效果对比图2](https://pic1.imgdb.cn/item/680d90ea58cb8da5c8cfda5f.png)

# 6.改进/挑战/问题/想法

* **想法**：这篇文章就是对投影层进行了微调。整体思路就是为了解决*Long VLLM* 问题，并且同时保证计算量不会很大，这个与之前看的一篇也是为了降低计算成本的文章在解决计算量问题处理方法类似，都是减少*tokens* 数量，这篇文章是通过双向软匹配方法减少，之前那篇文章是一个选择过程（选择留下哪些*tokens* ）。

