---
layout: post
title: 'Learning to Localize Actions in Instructional  Videos with LLM-Based Multi-Pathway  Text-Video Alignment ECCV 2024'
subtitle: '使用基于LLM的多路径文本-视频对齐学习定位教学视频中的动作'
date: 2025-03-12
author: Sun
cover: 'https://pic1.imgdb.cn/item/67cfa107066befcec6e2b56a.png'
tags: 论文阅读
---

> [Learning to Localize Actions in Instructional  Videos with LLM-Based Multi-Pathway  Text-Video Alignment](https://link.springer.com/chapter/10.1007/978-3-031-73007-8_12)
> 
> 1. ​**Rutgers University**​（罗格斯大学）
> 2. ​**Meta**​（Meta 公司，原facebook
> 3. ​**NEC Labs America - Princeton**​（NEC 美国实验室，普林斯顿分部）
> 4. ​**Michigan State University**​（密歇根州立大学）

# 1.文章针对痛点

这篇文章关注的是**教学视频中的动作定位**，这里的教学视频通常是指烹饪、修车、化妆等等。最近的工作关注的是**通过对比学习学习视频片段与ASR转录叙述文本之间的跨模态对齐**，这篇文章关注的也是这个方面。

但是，文章认为现有的方法存在以下问题：

1. 先前的工作在解决定位过程步骤任务时，要么以一种全监督的方式或若监督方式，但是<mark>这些方法往往需要大量精细的注释，而注释工作是昂贵且费力的。</mark>
2. 而为了解决上述的这个问题，现在的方法就提出了使用对比学习来学习叙述文本和视频之间的跨模态对齐；然后，这种方法是次优的，因为<mark>叙述在与视频内容对齐时，是包含噪声的。</mark>（这种噪声主要来自两方面，首先是一些叙述可能描述的信息与视频任务是不相关的；其次，在视频帧与来自叙述时间戳的叙述之间的时间相关性可能不是一致的，因为表演者可能在执行动作之前或者之后介绍）
3. 为了解决上述噪声的第一种，有工作提出使用认为构建的知识基础来定义过程步骤；但是这种方法可能不能一直对齐表演者的动作，因为任务可能以一种没有在知识基础中概述的方式执行。
4. 此外，可能没有考虑噪声对齐，例如，视频中教学任务不相关的叙述和叙述中不可靠的时间戳。

关于这些问题文章展示了一个图片可以直观感受。首先可以明显看到叙述的时间戳可能在动作执行之前、之后或执行中；另外，可以看到有的叙述是不相关的；此外，也可以看到执行的动作可能在知识基础中找到的过程步骤描述实际上是错误的，也就是图文不相关的。![问题](https://pic1.imgdb.cn/item/67cfa9ce066befcec6e2c3b5.png)

# 2.主要贡献

因此，为了解决上面的问题，文章提出了一种新的架构来训练过程步骤定位模型。首先文章利用*LLM* 来过滤不相关的信息，并从叙述中总结过程步骤；生成的过程步骤文章称为*LLM-steps* 。

此外，文章提出了**多路径文本-视频对齐策略**，来生成视频和*LLM-steps* 之间可靠的伪匹配。这个策略的关键是使用不同的路径来衡量视频和*LLM-steps* 之间的对齐，每一条路径从不同的方面来捕捉二者之间的关系，并提供互补信息，最后融合这些信息。
文章提出了三种路径：1️⃣**第一条路径是利用源自叙述时间戳的时间相关性**，首先识别与*LLM-steps* 语义相似的叙述，之后匹配每个*LLM-steps* 和视频片段（视频片段是与叙述语义相关的时间戳片段）。2️⃣**第二条路径是基于长期全局视频-文本语义相似性衡量对齐**。3️⃣**第三条路径是使用预训练在短时视频-文本数据集（来自各种领域）上的视频-文本基础模型，来直接对齐*LLM-steps* 和视频片段**；这条路径不仅捕捉细粒度、短时关系，还结合了从各种视频领域中学到的更广泛的知识。

# 3.实现流程

一样，还是先看一下原文展示的模型架构图：![模型架构图](https://pic1.imgdb.cn/item/67cfc7d9066befcec6e2ed5f.png)

文章所提出的方法整体来说就是包含三个部分，也就是架构图中的*a, b, c* 三部分。部分*a* ，就是使用*LLM* 处理叙述得到*LLM-steps*；部分b，使用多路径视觉-文本对齐生成伪匹配矩阵，而生成的伪匹配就作为标签，监督训练部分*c* 模型。

# 4.实现细节

* **使用LLM提取过程步骤** ：也就是对*LLM* 输入叙述和*prompt*，利用大语言模型强大的总结能力和任务过程理解能力，得到可用的*LLM-steps*。
* ***OOD* 边界推理**：这个模块是为了找寻*OOD* 和*ID* 查询之间的明确边界，同时为了减少计算，文章考虑基于不确定性分数的查询边界；而上一步计算的对数可能性分布式可以转化成不确定性分数。这一部分也包含两步：1️⃣通过正常化流，可以估计每个查询特征的对数可能性，如下公式1。最后得到的不确定性分数如下公式2。2️⃣得到不确定分数后，就可以推理得到*ID-OOD* 边界。首先基于公式2的*ID* 对数可能性，可以找寻*ID* 对数可能性分布；之后，通过这个*ID* 对数可能性近似所有查询的对数可能性分布；之后，引入一个位置超参数$$\alpha$$来确定边界，文章设置分类*ID* 对数可能性分布的第$$\alpha$$个百分位数作为*ID* 边界，也作为容错率的上限；最后为了增强模型的鲁棒性，文章还应用了*margin* 超参数$$\Delta$$，定义OOD边界$$b_{o o d}=b_{i d}-\Delta$$。

$$
\mathrm{log}p(q)=\sum_{c=1}^C\mathrm{log}|\mathrm{det}J_{\Phi_c}(k_{c-1})|-\frac{1}{2}\Phi_\omega(q)^T\Phi_\omega(q) \quad（1）
$$

$$
u(q)=\max_{q^{\prime}\in\boldsymbol{Q}}(\exp(\operatorname{logp}(q^{\prime})))-\exp(\operatorname{logp}(q)), \quad（2）
$$

* **多路径文本视频对齐**：提出这个策略是为了生成可靠的伪对齐来监督另外架构的训练。多路径包含三个部分：1️⃣对齐*LLM-steps* 和叙述，也就是计算二者之间的语义相似性，这部分计算如下公式1所示；之后再与视频对齐，如下公式2所示。2️⃣使用预训练在教学领域的长时视频-文本对齐模型直接对齐*LLM-steps* 和视频，需注意也就是直接将*LLM-steps* 和视频输入预训练模型，所以公式展示的编码器是预训练模型的。如下公式3所示。3️⃣使用预训练在短时视频-文本数据集（来自各个领域）直接对齐，实现步骤与第2条路径相似。最后将这三部分的对齐分数进行平均池化得到最终的分数矩阵。

$$
\mathbf{A}_{SN}=\mathrm{Softmax}\left(\frac{E_t(\mathcal{S})\cdot E_t(\mathcal{N})^\top}{\tau}\right) \quad（1）
$$

$$
\mathbf{A}_{SNV}=\mathbf{A}_{SN}\cdot\mathbf{Y}^{NV}\in\mathbb{R}^{L\times T} \quad（2）
$$

$$
\mathbf{A}_{SV}^{long}=E_t^L(\mathcal{S})\cdot E_v^L(\mathcal{V})^\top \quad（3）
$$

* **训练架构**：这部分就是按照正常的模型训练，只不过是监督学习的标签是由多路径学习生成的$$Y^{SV}$$。整个过程也就是先各自通过编码器编码，之后各自进入一个transformer，大概就是增强特征；最后使用联合模态的transformer来融合多模态信息。



# 5.模型性能

实验性能效果对比图：![效果对比图1](https://pic1.imgdb.cn/item/67d0f522066befcec6e37df6.png)

# 6.改进/挑战/问题/想法

* **想法**：这篇文章其实解决的核心问题，我认为应该还是在文本-视频对齐上，其他的内容都是常规做法。

