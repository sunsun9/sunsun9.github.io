---
layout: post
title: 'Online Temporal Action Localization with  Memory-Augmented Transformer ECCV2024'
subtitle: '具有记忆增强Transformer的在线时间定位'
date: 2025-02-23
author: Sun
cover: 'https://pic1.imgdb.cn/item/67b93299d0e0a243d401b7ec.png'
tags: 论文阅读
---

> [Online Temporal Action Localization with  Memory-Augmented Transformer](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02834.pdf)
> 
> 1 Pohang University of Science and Technology (POSTECH), South Korea；

# 1.文章针对痛点

这篇文章关注的是在线时间动作检测，与离线的时间动作定位不同，该任务是根据当前的时间片段预测动作开始、结束时间以及动作类别，并且一旦动作实例被预测，后续不允许修改结果。

文章认为现有的方法每次迭代仅仅是将固定大小的视频片段作为输入，因此这些方法在考虑长期上下文时受限，并且需要小心微调片段大小。（但是文章提出这个点很奇怪，因为后面文章也是使用的固定长度的滑动窗口）

文章也提到现有的方法现在锚框*transformer（OAT）*，这种方法在在线时间动作检测任务上取得了一定的效果，但是仍存在一些问题；首先，因为在每个迭代采用固定长度片段作为输入，所以没有考虑长期上下文，在检测长动作实例时能力有限；其次，对输入片段尺寸大小超参数很敏感。

# 2.主要贡献

为了解决上面的问题，文章提出了一种新的端到端的架构，*memory-augmented Transformer (MATR)*。这个方法的核心是内存队列，内存队列有选择地存储过去的片段特征，从而让模型为推理利用长期上下文。

此外，文章使用内存队列，为动作实例定位提出了一种新的方法。这个方法首先使用当前片段特征定位动作的结束时间；然后在内存队列中浏览过去片段特征，来找寻检测动作结束时间对用的动作开始。

原文总结本文贡献如下：

* 提出了*MATR*，这是一种新的端到端体系结构，用于实现内存队列，利用内存队列和长期上下文实现了对动作实例的精确定位，保证模型对数据集特异性超参数的依赖性较小。
* 引入了一种新的动作实例定位方法，该方法首先使用输入段来标识动作结束，并在内存队列中扫描过去的信息以查找动作开始。此外，文章的方法采用不同的查询来将信息分开以进行动作分类和定位。
* 文章方法优于两个基准测试的现有在现有方法。同样，广泛的消融研究证明了所提出方法的每个组成部分的贡献。

# 3.实现流程

一样，还是先看一下原文展示的模型架构图：![模型架构图](https://pic1.imgdb.cn/item/67b9574ad0e0a243d401c938.png)

**文章提出的模型主要包含四部分，分别是特征提取器、记忆增强视频编码器、实例解码模块和预测头。**

特征提取是通过**视频*backbone* 网络和线性投影层实现的**；获取特征后，这些特征**被送入记忆增强视频编码器**，**在当前片段编码帧之间的时间上下文信息，并存储片段特征到内存中**；之后，实例**解码模块**会通过两个*Transformer* 解码器（开始解码器和结束解码器）来**定位动作实例**；最后解码模块的输出被送入预测头，得到最后的预测结果。

# 4.实现细节

* **特征提取**：这个部分比较简单，就是使用一个*backbone* 和一个线性投影层提取特征。
* **记忆增强视频编码器**：在每个时间步，输入片段特征都会被送入记忆增强视频编码器的两个模块，片段编码器和记忆更新模块；前者就是编码输入片段特征的时间上下文，后者就是有选择存储片段特征，并更新内存队列。
  ***（1）片段编码器***：该编码器是标准*Transformer* 编码器，包含自注意力层和一个前馈神经网络。**片段特征和可学习*flag token* 的结合体**被送入该编码器，被转化成查询、键、值（正弦位置编码也添加到了查询和键中）。在输出嵌入空间，编码的*flag token* 被送入记忆更新模块，**编码的片段特征作为结束解码器的输入**。
  ***（2）记忆更新模块***：在片段编码器过程中加入了*flag token*，这个*token* 就是决定当前片段是否应该进入内存队列。因此，*flag token* 会被送入*flag* 预测头；<mark>如果 flag=1 ，那就说明这个片段与动作实例是具有重合部分的，应该进入内存队列；如果 flag=0 ，就不进入</mark>。（注：如果队列已经满了，就丢弃旧的存储）
* **实例解码模块** ：在得到特征后，模型需要对特征进行解码，得到最后的预测结果。文章将实例解码模块分为了结束解码器和开始解码器，二者的结构是一样的，只是输入的信息不一样。文章展示的这一部分的架构图如下：![架构图](https://pic1.imgdb.cn/item/67b96e5ed0e0a243d401d285.png)
* **预测头** ：预测头也是包含三个部分，结束预测头、开始预测头和动作类别头，每一个预测头都包含2层的*FFN*。

# 5.模型性能

这种在线的时间动作定位，就是利用现有的信息预测将来的动作其实差不多，所以这个准确性要达到很高的程度，还是需要结合一些其他的内容来帮助模型预测未来。所以这个模型的性能其实与离线的时序动作检测还是有很大的差距的。方法在数据集上的效果：![数据集](https://pic1.imgdb.cn/item/67ba8cacd0e0a243d4025de5.png)

# 6.改进/挑战/问题/想法

* **想法**：就是这个内存队列是一个可学习的地方吧，其实感觉和*LSTM* 这种比较类似吧，只不过说只要在这个队列里面，这个存储的信息是不会变得，但是*LSTM* 是一直在对过去的信息进行从操作。
* **改进**：上面也是提到了，这个任务其实就是有点预测未来的感觉，但是这个是不太容易的。如果是专业性动作的话，那么动作转变切换会相对容易预测；但是如果是生活向的动作，这个切换以及转换太多变了。所以在这方面，我认为如果要实现比较好的效果，应该有个什么可以去辅助模型预测，为预测提供一定的参考。

