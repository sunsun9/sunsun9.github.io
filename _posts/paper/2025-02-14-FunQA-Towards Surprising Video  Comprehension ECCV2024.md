---
layout: post
title: 'FunQA: Towards Surprising Video  Comprehension ECCV2024'
subtitle: 'FunQA: 迈向令人惊讶的视频理解'
date: 2025-02-14
author: Sun
cover: 'https://pic1.imgdb.cn/item/67aec7fad0e0a243d4ff0d34.png'
tags: 论文阅读
---

> [FunQA: 迈向令人惊讶的视频理解](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00010.pdf)
> 
> 1北京邮政与电信大学，中国北京；2 哈利法大学，阿布扎比，阿联酋；3 S-LAB，Nanyang Technological University，新加坡；4 艾伦AI，华盛顿州艾伦AI研究所

> 这篇文章主要就是为了提出一个新的基准，并且使用现有的模型在这个基准上评测；因此这篇文章其实并没有提到一些新的模型架构等等，看这篇文章本来是想看看关于时序动作定位这个部分是怎么实现的，但是这篇文章并没有关于模型架构的部分，仅仅是利用现有的一些*VLM* 模型进行评测。所以后面的实验流程、实现细节并没有继续关注了。
> 
> 但是提出的*Funmentor* 还是有一定的参考价值吧，如果想利用大语言模型为自己的工作生成一些有效的提示的话，其实是可以用一用的。

# 1.文章针对痛点

这篇文章关注的视频理解问题，主要是视频问答任务。视频问答任务也就是模型读取一段视频，用户提出一些问题，模型能够基于理解给出问题的答案。

文章任务现有的大部分视频问答基准都是较好理解的，例如做饭或者教学视频；很少基准关注的是违反直觉的视频，例如文章研究的幽默视频、创造性视频和魔术视频，文章认为这种需要一定理解能力的视频具有很大的挑战性。也就是模型需要理解什么是幽默，人为什么会认为这个是幽默的。

下面展示了文章关于这个任务的一个简述图：![简述图](https://pic1.imgdb.cn/item/67aeca66d0e0a243d4ff0d9f.png)

从上图中可以看到，在文章所关注的任务中，也有关于时序动作定位任务的研究，即根据文字提示给出文字提示所关注动作的时间间隔。

# 2.主要贡献

这篇文章的主要贡献就是提出了一个**新的视频问答基准*FUNQA***，并且在新基准中，制定了三个严格的任务，以衡量模型对*surprise* 的理解：1）反直觉的时间戳定位：模型必须在发生意外事件时确定视频中的特定时间段。 2）详细的视频描述：模型必须生成视频内容的连贯和客观描述，并评估模型的基本视频理解能力。 3）违反直觉的推理：模型必须生成具体解释，说明视频令人惊讶。

其次，**文章引入了*Funmentor***，这是一种旨在提高*VLM* 中违反直觉推理的专业人士。像经验丰富的教练在综艺节目中一样，*Funmentor* 进行了详细的，多转的对话，磨练了模型的响应，以准确掌握有趣和令人惊讶的内容的本质。


原文将文章贡献总结为以下内容：

* 新的*VideoQA* 数据集：构建了一个大规模数据集*FUNQA*，该数据集*FUNQA* 可以用有趣的视频补充现有的*VideoQA* 数据集。
* 新颖和具有挑战性的任务：设计了许多新任务，使该模型可以探索以前未触及的问题，例如时间戳定位以及围绕反智能的推理。这些任务将视频推理推向了肤浅的描述，要求更深入地理解。
* 新型方法筹集者：提出了一个新的代理，通过与*VLMS* 的多圈对话来完善模型对反智能的理解。
* 全面评估：对尖端基线进行了全面的评估，使该领域有了洞察力和未来的研究方向。

# 3.实现流程
# 4.实现细节

# 5.模型性能
# 6.改进/挑战/问题

* **改进**：突然想到，其实这个文章利用*VLM* 模型可以实现时序动作定位任务，那么是不是也可以在自己的工作中结合这个，比如把这个*VLM* 模型输出的时间间隔作为提示或者一个共同学习的特征，感觉这也可以是一个较好的方向。后面可以尝试这个试一下。



