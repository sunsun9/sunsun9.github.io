---
layout: post
title: 'ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking ICCV2025😊'
subtitle: 'ATCTrack: 面向鲁棒的视觉-语言跟踪 对齐目标上下文线索和动态目标状态'
date: 2025-09-02
author: Sun
cover: 'https://pic1.imgdb.cn/item/68b5341058cb8da5c869c07c.png'
tags: 论文阅读
---

> [ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking](https://arxiv.org/abs/2507.19875)

> 💐💐[仓库](https://github.com/XiaokunFeng/ATCTrack)
> 
> 📌作者单位
> 
> 1. 中科大
> 2. 中国科学院自动化研究所
> 3. 南洋理工大学

# 1.文章针对痛点

这篇文章关注的是视觉-语言跟踪，这个任务是在第一帧给定一个文本（文本仅描述该帧目标对象的状态）和一个模板块（只是包含了目标对象，但是并没有边界框），根据给定的提示和视频，定位后面视频内容对应的目标对象。

由于任务设置的关系，<mark>提供的文本描述并不总是与目标对象的状态一致，也就是说在视频场景的变化下，目标对象的状态是持续改变的</mark>；因此，仅依靠提供的目标线索是远远不够的。并且当目标的状态发生改变时，提供的线索反而是错误的提示；虽然现在有一些方法意识到这个问题，并针对这个问题处理减少干扰的文本描述，但是由于缺少监督信息并且文本形式多样，文章发现他们采用的简单方法（利用视觉-文本相似性）不能达到满意的效果。

现有的很多方法都是仅依赖提供的线索，虽然可以取得一定的效果，<mark>但是这种模式会忽略目标对象一些固定的动态特征，比如人物的性别、衣服等等。</mark>

# 2.主要贡献

针对上面的问题，文章提出了一种新的跟踪方法*ATCTrack*，该方法可以有效建模目标上下文特征，从而获得和动态目标状态对齐的多模态线索。具体而言，**对于视觉模态**，文章设计了一种有效的时间视觉目标 - 上下文建模方法，为跟踪器提供及时的视觉提示；通过明确构建目标 - 上下文空间分布图并将其集成到更新的时间内存中。**对于文本模态**，首先仅基于文本内容提出一种精确的目标单词意识方法。也就是即使不依赖视频数据，也可以纯粹从文本中识别目标单词。与依赖文本单词和视觉目标之间细粒度对齐的现有方法相比，文章认为这种方法可以简化任务，并实现了令人印象深刻的目标单词分类精度。鉴于现有基准中缺乏句子组件标签，文章通过利用现成的大语言模型来设计一个自动目标词注释管道，以提供监督标签进行模型培训。此外，利用准确识别的目标词，设计了一种创新的上下文校准机制，以减轻上下文词提示的潜在误导性影响。

设计对比图:![设计对比图](https://pic1.imgdb.cn/item/68b5391758cb8da5c869ec98.png)

原文总结贡献如下：

1. 我们提出了一个名为*ATCTrack* 的新型跟踪器，该跟踪器可以通过全面的目标 - 上下文特征建模来获得与动态目标状态对齐的多模式线索。与通常仅与初始目标状态保持一致的初始提示的局限性相比，这些对齐的提示可以更牢固地指导跟踪。
2. 对于视觉模态，*ATCTrack* 有效地对时间视觉目标角色特征进行建模以捕获及时的视觉提示。对于文本方式，*ATCTrack* 实现了目标单词的精确意识，并减轻了上下文单词的潜在误导性影响。
3. 我们对主流基准进行了广泛的实验，并且*ATCTrack* 实现了新的*SOTA* 性能。

# 3.实现流程

文章提出模型的架构图如下所示，整体而言就是分成了视觉和文本两个大模块。文本编码器和视觉编码器首先将它们编码为特定的特征空间。然后，文本目标 -上下文指导模块和视觉目标 - 上下文指南模块顺序建模与动态搜索目标对齐的全面文本和视觉目标上下文特征，并将它们嵌入搜索功能中。在此过程中，内存存储模块（*MSM* ）提供了存储的视觉内存功能并保存更新的内存信息。最后，预测头用于基于嵌入式搜索功能获得跟踪结果。

![模型架构图](https://pic1.imgdb.cn/item/68b53eff58cb8da5c86a29d2.png)

# 4.实现细节

* **编码器**：1️⃣视觉编码器。针对每个时间步，视觉编码器的输入包含三部分，分别是搜索图像（即待处理的图像）、模板图像（即第一帧用户提供的）和动态目标状态（根据前面的跟踪结果模型自己整理的）。这三部分经过视频编码器处理后得到$$f_x^t$$，这个就是搜索图像的编码结果，和模板图像的编码结果$$f_z^t$$，以及一个可学习的$$[CLS]$$ *token* 来捕获全局视觉语义特征。2️⃣文本编码器。这个没有什么特殊的处理，就是正常编码文本特征。
* **文本目标-上下文引导模块**：为了充分利用文本提示，<mark>该模块的关键是仔细调整初始文本提示，以充分专注于目标单词并减轻上下文单词的潜在误导性效果</mark>。如架构图所示，首先明确地从文本内容中识别目标词。然后，设计了一个上下文词校准机制，该机制利用已确定的目标词和内存存储模块（*MSM* ）提供的视觉内存功能来有效利用上下文词。最后，我们将调制文本功能与搜索功能集成在一起，以实现文本提示的指导。1️⃣目标单词感知。文章将这个任务处理为一个二分类任务，也就是判断当前文本句子的每个单词是否属于目标单词；为了监督训练，文章使用大语言模型生成了标签。2️⃣上下文单词较准。上下文单词和目标状态之间的一致性决定了它们是指导还是误导跟踪器。建模过程需要*MSM* 存储的时间内存特征，该特征代表最新的视觉目标信息信息。此外，由于上下文信息取决于目标的位置，因此准确地感知目标功能有助于捕获精确的上下文信息。整体的工作流程就是架构图(b)所示的流程，首先将目标单词感知得到的目标单词与记忆模块中的特征进行*concate*，如下公式1所示，其中$$M_*^{t-1}$$是$$M^{t-1}$$中组件的*concate* ；之后进行交叉注意力操作，文章描述的是将第一个元素作为查询，第二个元素作为键和值，公式如下2所示；之后就是与初始的文本进行交叉注意，让模型具有自适应调整上下文单词的能力，如下公式3所示。3️⃣文本线索引导。在这一步将第二步最后得到的$f_{L^`}$与$$f_L$$进行*concatenate* 操作，最后与$$f_X^t$$进行交叉注意力操作，得到$$f_{XL}^t$$。
  
  $$
  f_{LM}=[f_{LT};M_{*}^{t-1}]	\quad(1)
  $$
  
  $$
  f_{LM^{\prime}}=Norm(f_{LM}+\Phi_{CA}(f_{LM},f_{LM}))	\quad(2)
  $$
  
  $$
  f_{L^{\prime}}=Norm(f_{L}+\Phi_{CA}(f_{L},f_{LM^{\prime}}))	\quad(3)
  $$
* **视觉目标-上下文引导模块**：该模块由两个核心过程组成：视觉记忆表示和指导，旨在建模和利用与目标状态对齐的动态视觉目标 - 内存记忆。1️⃣视觉记忆表示。虽然前面已经通过视觉编码器获得了当前时间步的整体表示，但是文章认为这个信息不足以表示准确的目标位置信息，即缺乏准确的目标上下文感知。因此文章通过热图辅助构建，具体而言，通过计算待处理图像与模板图像特征之间的相似度，得到热图，公式如下1所示；之后，与记忆模块存储的内容进行一系列操作得到视觉记忆表示$$m^t$$，如下公式2 3所示。2️⃣视觉记忆引导。文章进行了逐元素的注意力操作得到$$f_R^t$$，如下公式4所示。

$$
h^t=(f_X^t\cdot(f_Z^t)^T).mean(dim=1)	\quad(1)
$$

$$
f_{[C]M^{\prime}}=Norm(f_{[C]M}+\Phi_{CA}(f_{[C]M},h^t\odot f_X^t))	\quad(2)
$$

$$
f_{[C]M^{\prime\prime}}=Norm(f_{[C]M^{\prime}}+FFN(f_{[C]M^{\prime}}))	\quad(3)
$$

$$
f_R^t=softmax(\frac{f_{XL}^t\cdot(m^t)^T}{\sqrt{D}})\cdot m^t \quad(4)
$$


# 5.模型性能

![模型性能1](https://pic1.imgdb.cn/item/68b65c9358cb8da5c86e9755.png)

# 6.改进/挑战/问题/想法

* **想法**：

