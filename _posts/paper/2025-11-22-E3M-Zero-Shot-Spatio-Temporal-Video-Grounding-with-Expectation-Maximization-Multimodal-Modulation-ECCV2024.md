---
layout: post
title: 'E3M: Zero-Shot Spatio-Temporal Video Grounding with Expectation-Maximization Multimodal Modulation ECCV2024😐'
subtitle: 'E3M: 具有期望最大化多模态调制的零样本时空视频定位'
date: 2025-11-22
author: Sun
cover: 'https://pic1.imgdb.cn/item/692149473203f7be001ff3b4.png'
tags: 论文阅读
---

> [E3M: Zero-Shot Spatio-Temporal Video Grounding with Expectation-Maximization Multimodal Modulation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11011.pdf)

> 🌟🌟[项目主页 代码未上传]([GitHub - baopj/E3M: [ECCV 2024\] The first zero-shot setting for spatio-temporal video grounding.](https://github.com/baopj/E3M))
>
> 📌作者单位
>
> 1. 南洋理工大学
> 2. 北京大学
> 3. Peng Cheng Laboratory

# 1.文章针对痛点

现有的时空视频定位方法基本都是全监督或者弱监督，而这两种形式都需要进行训练。但是时空视频定位任务需要逐帧的细粒度标注，是十分费时费力的工作；此外，在真实开放场景下，很多训练数据是无法获取的。

而对于零样本时空视频任务而言，**一个重要的挑战是不仅仅需要关注帧级的理解，也需要整合推理时空语义信息**。

# 2.主要贡献

为了克服这些挑战，文章提出了一种**多模态调制算法**来促进时空理解，通过**整合时空上下文来增强视觉和文本表示**。具体而言：

* 在视觉方面，文章设计了一种**基于上下文的视觉调制**，旨在通过上下文语义增强对象实例的视觉表示。首先通过卡尔曼滤波器将对象实例的空间信息沿时间维度传播到相邻帧。然后，自适应地聚合这些传播上下文的视觉特征，从而形成富含综合上下文信息的时空表示。
* 在文本方面，文章提出了一种**基于原型的文本调制**，用视觉时空语义补充文本表示，从而弥合它们的跨模态差异。首先从与句子语义密切相关的时空视觉特征中识别语义原型。随后，这些语义原型被用来校准文本特征，有效地将时空信息封装在文本域内。
* 此外，*STVG* 本质上呈现出一种**相互交织的时空困境**：视频中的空间基础取决于精确的时间基础，因为视频包含与查询相关的正帧和负帧。时间失误可能会无意中导致不相关帧内的空间推理，从而损害整体准确性。相反，考虑到帧中存在多个不相关的实例，时间基础的有效性也取决于空间识别。为了解决这种相互交织的困境，文章将时间相关性分数视为潜在变量，并引入期望最大化（*EM* ）框架，以交替范式操作时间和空间基础。



原文总结贡献如下：
1.我们提出了一种免训练算法，以零样本的方式解决 *STVG* 任务，消除了任何训练视频或注释的必要性。据我们所知，这是零样本 *STVG* 的首次尝试。

2.为了促进时空推理，我们提出了期望最大化多模态调制*E3M* 算法，该算法包括基于上下文的视觉调制、基于原型的文本调制和迭代范式中的*EM* 优化。

3.大量实验验证了我们的零样本方法优于一系列对两个大型数据集（即 *HC-STVG*  和 *VidSTG* ）使用更强监督的最先进方法。

# 3.实现流程

文章提出了一种零样本 *STVG* 的期望最大化多模态调制 *E3M* 算法，架构图如下所示。具体来说，通过引入了期望最大化框架来优化交替范式中的时间和空间推理。**表示句子与每个视频帧之间的相关性的时间相关性分数被视为潜在变量**。

* 在 *E* 步骤中，利用 *M* 步骤的空间基础结果来估计时间相关性分数的分布。
* 基于估计的时间相关性分数，*M* 步骤然后通过（1）基于上下文的视觉调制，（2）基于原型的文本调制来优化空间基础结果
* 基于上下文的视觉调制在视觉前沿进行操作，旨在实现时空上下文中的视觉推理。对象实例的区域信息首先通过卡尔曼滤波器沿时间维度传播。然后聚合传播区域的视觉特征，以通过时空上下文增强对象实例。同时，在文本方面，设计了基于原型的文本调制来弥合视觉文本差异。首先从与句子密切相关的视觉特征中识别语义原型；然后，通过合并这些视觉语义原型来重新校准这些文本特征。
  ![架构图](https://pic1.imgdb.cn/item/69214d983203f7be00201180.png)

# 4.实现细节

**M STEP**：假设在期望*E* 步骤中估计时间相关性γ ∈ R T 的分布，该分布表示*T* 个视频帧和查询*Q* 之间的相关性。基于时间相关性分布*γ* ，最大化*M* 步骤旨在通过联合时空理解来优化空间基础结果。为了实现这一目标，文章设计了一种多模态调制算法，该算法集成了基于上下文的视觉调制和基于原型的文本调制。

1️⃣基于上下文的视觉调制：这个方法首先利用目标检测器检测每帧的空间定位框，单帧检测到的空间定位框可能有多个；而为了找到一个最合适的框，文章首先将视觉特征进行了分离，即分离目标区域和背景区域。之后将视觉送入*CLIP* 中进行单帧编码，因为是单帧独立进行的，不具备时序能力；所以文章使用卡尔曼滤波器将单帧特征传递到相邻帧，从而让特征具备时空上下文信息（具体信息参照原文）。**总体而言就是通过卡尔曼滤波器让特征具有上下文信息，从而帮助模型在M步骤选择最合适的空间框。**

2️⃣基于原型的文本调制：为了应对视觉文本语义差异带来的挑战，文章提出了一种新颖的基于原型的文本调制，它通过利用互补语义原型来完善文本表示。文章期望语义原型满足以下两个属性。 1）范例性：每个原型应与句子的语义内容紧密结合。 2）多样性：原型应该代表动作的各个阶段，这样它们就可以捕捉视觉语义的演变本质，并补充文本表示中的差异。提供了伪代码如下：![伪代码](https://pic1.imgdb.cn/item/6921529f3203f7be00202907.png)

**E STEP**：主要是利用*M* 步骤的结果来去噪

```
# 1. 用空间结果去噪：模糊无关实例，只保留目标
blurred_frame = blur_irrelevant_instances(frame, grounding_result)

# 2. 提取时空视觉特征
v_t* = context_based_visual_modulation(blurred_frame)

# 3. 计算时间相关性
γ_t = cos_sim(q, v_t*) + cos_sim(q, v_t)  # 融合帧级和实例级

# 4. K-Means聚类形成原子事件
clusters = kmeans(visual_features + time_embedding)

# 5. 选择最相关的原子事件并合并
best_cluster = argmax(avg_relevance(cluster))
merge_adjacent_if(relevance > β * best_relevance)
```



> [!NOTE]
>
> 总结这篇文章从实现细节来看，重点还是在怎么从空间的多个候选框中选择一个框，所以也可以看出来空间设计相对时间设计要复杂一些；而时间设计相对简单，仅仅利用空间进行去噪。其实空间也只是利用时间的估计分数作为一个重要的参考信息，这篇文章的主要设计点我认为应该是在设计怎么构建利用上下文信息上。



# 5.模型性能

![性能对比图](https://pic1.imgdb.cn/item/692153fd3203f7be0020326d.png)

# 6.改进/挑战/问题/想法

* 参考了解