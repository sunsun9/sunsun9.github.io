---
layout: post
title: 'Weakly-Supervised Spatio-Temporal Video  Grounding with Variational Cross-Modal  Alignment ECCV 2024'
subtitle: '具有可变跨模态对齐的弱监督时空视频定位'
date: 2025-04-09
author: Sun
cover: 'https://pic1.imgdb.cn/item/67eca8d40ba3d5a1d7e9cc2c.png'
tags: 论文阅读
---

> [Weakly-Supervised Spatio-Temporal Video  Grounding with Variational Cross-Modal  Alignment](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06485.pdf)

> ❌❌不提供代码
> 
> 📌作者单位
> 1.北京大学

# 1.文章针对痛点

这篇文章关注的是弱监督设置下的时空视频定位任务。

弱监督设置下的文章基本认为的问题都是全监督的注释工作是费时费力费钱的，所以全监督的方式很难扩展到大规模数据集上。

而弱监督由于缺乏注释并且视频内容是随时间变化的，因此弱监督是存在一些独特的挑战的。具体来说，首先由于视频帧中存在具有相似视觉特征或者执行类似动作的实体，要**实现区域和短语（文章提出的子问题）的准确对齐是复杂的**；其次，相比于图像中的静态视觉关系，**视频中的交互式动态的**，并且随着时间会发生改变或消失。

# 2.主要贡献

所以针对上面的这些问题，文章先在弱监督设置下提出了一种新的架构。具体来说，文章首先将文本解析成名词短语，并提取区域提案作为候选框；之后将*STVG* 任务分为两个跨模态对齐任务，分别是区域-短语和帧-句子，其中前者是准确识别短语表示的区域，后者是检测与句子语义表达对应的帧。

此外，文章考虑到缺少*ground truth*，因此将他们视为潜在变量，并学习以视频和句子对应关系为条件的联合分布。

整个框架采用*variational  Expectation-Maximization (EM) algorithm* 实现高效优化。

原文总结贡献如下:

1. 本文探讨了具有挑战性的弱监督环境下的 *STVG* 任务。具体来说，我们创新性地将时空接地分为两个不同的跨模态配准子问题，并通过有效的潜在变量对其进行表述。
2. 我们提出了一种采用变异期望最大化算法来优化整个框架的方法。值得注意的是，所提出的模型在训练过程中无需人工标注就能成功实现精细的跨模态配准。
3. 我们在两个大型挑战性视频基准（即 *VidSTG* 和 *HC-STVG* ）上进行了广泛的实验。结果表明，我们提出的方法大大超越了现有的弱监督方法，从而证明了其优越性。

# 3.实现流程

一样，还是先看一下原文展示的模型架构图：![模型架构图](https://pic1.imgdb.cn/item/67ecc70c0ba3d5a1d7e9f6e8.png)

# 4.实现细节


# 5.模型性能

文章的实验部分不仅仅包含文本-图像的实验，也进行了图像-文本的实验。

实验性能效果对比图。![效果对比图1](https://pic1.imgdb.cn/item/67f62e6088c538a9b5c77af7.png)
![效果对比图2](https://pic1.imgdb.cn/item/67f62e8c88c538a9b5c77b3b.png)

# 6.改进/挑战/问题/想法

* **想法**：这篇文章的效果相比于之间看的一篇开放集的效果差挺远的。最近一直在改代码，实在不想看论文，因为卡占满了，没轮上，所以把这个写完了，实现细节实在看不进去了😞😞

