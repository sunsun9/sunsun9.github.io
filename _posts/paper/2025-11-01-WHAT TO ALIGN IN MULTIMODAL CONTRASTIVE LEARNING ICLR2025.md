---
layout: post
title: 'WHAT TO ALIGN IN MULTIMODAL CONTRASTIVE LEARNING? ICLR2025😊'
subtitle: '在多模态对比学习中需要对齐什么'
date: 2025-11-02
author: Sun
cover: 'https://pic1.imgdb.cn/item/6905a2403203f7be00bec973.png'
tags: 论文阅读
---

> [WHAT TO ALIGN IN MULTIMODAL CONTRASTIVE LEARNING?](https://arxiv.org/abs/2409.07402)

> 💐💐[github地址](https://github.com/Duplums/CoMM)
> 
> 📌作者单位
> 
> 1. EPFL
> 2. NeuroSpin, CEA
> 3. CEDRIC, CNAM
> 4. Radiology Department, CHUV

# 1.文章针对痛点

这篇文章认为现在的多模态学习多是基于多模态之前有大量的冗余信息，但是文章认为不同模态之前除了这些共有的冗余信息之外，其他的信息也值得在多模态之间进行交互。

# 2.主要贡献

为了实现与任务无关的多模态学习，文章提出了一种多模态对比学习自监督预训练方法*CoMM*，该方法能够在单个多模态空间中进行模态通信。与之前施加跨模态约束来对齐单模态表示的对比多模态方法不同，文章利用简单的多模态架构将多模态输入融合到通用表示中，然后通过最大化这些特征的增强版本之间的互信息来对齐多模态特征。

*CoMM* 能够在多模态表示学习的背景下对多模态交互进行建模，包括冗余、唯一性和协同作用，这些术语可以自然地文章的对比多模态公式中出现。通过类比，*CoMM* 考虑由模态特定处理器的并行流构建的共享表示空间。当任务需要来自给定模态或模态之间交互的知识时，应仅使用表示空间的这些部分。

---

1. **突破“多视图冗余假设”的局限**

* 传统多模态对比学习方法（如CLIP）假设不同模态之间包含的信息是冗余的，即每个模态都包含完成任务所需的所有信息。
* 本文指出这种假设忽略了多模态之间的​**独特性（Uniqueness）**和**协同性（Synergy）**​，限制了模型对复杂多模态交互的建模能力。

2. **提出 CoMM 框架：对齐多模态表示而非跨模态表示**

* CoMM 不再分别对齐图像和文本等单模态表示，而是先将多模态输入融合为一个统一的多模态表示，再对该表示进行对比学习。
* 通过最大化增强版本之间的互信息，​**自然地将冗余、独特和协同信息建模进统一表示中**​。

3. **基于信息论的理论基础**

* 利用 **Partial Information Decomposition（PID）** 理论，从信息论角度分解多模态信息为：
  * 冗余（Redundancy）
  * 独特性（Uniqueness）
  * 协同性（Synergy）
* 理论上证明了 CoMM 能够学习这三种信息，而传统方法只能学习冗余信息。

4. **统一的融合架构与对比目标**

* 使用 Transformer 融合不同模态的特征，设计统一的对比目标，支持任意数量的模态。
* 在多个真实多模态任务上取得 SOTA，验证了其通用性和可扩展性。

# 3.实现流程

多模态模型架构图如图所示：为了捕获多模态交互，该模型主要由三个组件组成： **编码器**。每个模态由 $$n$$ 个模态特定编码器之一独立编码。**潜在转换器**。将特征转换为特定于模态的嵌入序列的线性模块。在潜在转换器之后，串联操作收集这些序列以送到**变压器架构**中。**变压器块**。该模块的目标是通过 *Transformer* 块中的多头自注意力层执行特定模态嵌入的融合，获得最终的多模态嵌入 $$Z$$。
![多模态模型架构图](https://pic1.imgdb.cn/item/6905a7af3203f7be00bee48f.png)

# 4.实现细节

# 5.模型性能

![性能1](https://pic1.imgdb.cn/item/6905a8ed3203f7be00bee75e.png)

# 6.改进/挑战/问题/想法

* **想法**：

