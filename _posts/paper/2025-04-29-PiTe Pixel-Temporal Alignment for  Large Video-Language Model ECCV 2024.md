---
layout: post
title: 'PiTe: Pixel-Temporal Alignment for  Large Video-Language Model ECCV 2024'
subtitle: 'PiTe: 面向大视频语言模型的像素时间对齐'
date: 2025-04-29
author: Sun
cover: 'https://pic1.imgdb.cn/item/680ee7b258cb8da5c8d0dd92.png'
tags: 论文阅读
---

> [PiTe: Pixel-Temporal Alignment for  Large Video-Language Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00744.pdf)

> ❌❌[不提供代码]
> 
> 📌作者单位
> 
> 1. 西湖大学
> 2. 苏州大学

# 1.文章针对痛点

这篇文章关注的问题是视频大语言模型，关注重点在于多模态对齐上。

为了对齐文本与视频模态，常见的途径是将视频特征对齐到文本特征的嵌入空间中，而现有的方法为了实现这种方法，大都采用指令调整的方式。而文章<mark>认为简单的问答训练范式主要是帮助*LLMs* 从空间角度理解视频数据，在有效捕捉时间动态和空间一致性上仍存在很大的挑战。因此，认为仅就仅利用指令调整的方式不足以全面理解视频</mark>。

也就是关注重点在于实现好的多模态对齐，从而能够帮助模型更好的理解视频内容。

# 2.主要贡献

所以针对上面的内容，文章提出了*PiTe*，一种新的大视频语言模型。该模型利用轨迹来实现跨空间和时间维度在像素级上复杂的视频和文本对齐。

要求模型预测文本中提到的单个物体在视频中的运动轨迹，通过利用视频上下文的时间维度来实现细粒度文本到像素的对齐学习，并增强其基于证据生成输出的能力。

由于没有现成的具有物体移动轨迹的视频语言数据集，文章通过自动标注管道策划了一个大规模视频语言数据集 *PiTe-143k*。所提出的 *PiTe* 极大地增强了 *LVidLM* 全面理解视频的能力，从而在零样本条件下的问题解答、时间定位和密集字幕任务中取得了有前途、有竞争力和最先进的性能。

原文总结贡献如下：

1. 我们通过自动标注管道，为所有单个对象策划了一个带有轨迹的大规模视频语言数据集 *PiTe-143k*。
2. 我们提出了一种新颖的 *LVidLM PiTe*，利用轨迹在空间和时间维度上调整视频和语言特征。
3. 在零样本视频问题解答、时间定位和密集字幕任务的无数数据集上进行的大量实验结果和分析证明了 *PiTe* 的优越性。

# 3.实现流程

展示了在训练时，模型包含的三个阶段（因为文章并没有展示架构图，而是在展示了训练阶段，但是在训练阶段中，也可以看出模型大概是什么样的架构，不过这种大视频语言模型的架构其实都差不多）：
![模型架构图](https://pic1.imgdb.cn/item/680f2c1958cb8da5c8d203dd.png)

也就是文章提出的模型*PiTe* 架构相对简单，包含一个视觉编码器，文章使用的*ViT*；一个视觉*adapter*，文章使用的是线性投影层，用于将视觉特征投影到文本特征所在的嵌入空间；一个*LLM Vicuna v1.5*，应该就是使用到的大语言模型；一个定位投影器或者轨迹投影器，用于引导*LLMs* 理解视觉信息，文章使用的也是一个线性投影层。

针对上面几个模块是做什么的，简单介绍。**视觉编码器**这个就不用说了，就是提取采样帧的视觉特征，具体如下公式1；**视觉*adapter*** 文章采用的投影全局特征，如下公式2，将所有投影后的特征组成一个序列，得到LLMs的能理解的输入$$\mathbf{z}=\{z_1,z_2,\ldots,z_N\}\in\mathbb{R}^{N\times d}$$；**大语言模型**处理传来的视觉和文本特征，并得到最后的回答结果，如下公式3 4。

另外，需要说明的是文章还创建了一个数据集，这个数据集文章有进行简单介绍，这里就不再阐述，我主要关注的文章怎么进行的多模态融合。

$$
\left\{v_{i}^{c l s}, v_{i}^{1}, v_{i}^{2}, \ldots, v_{i}^{P}\right\}=\operatorname{ViT}\left(f_{i}\right),	\quad(1)
$$

$$
z_i=\mu\left(v_i^{cls}\right)	\quad(2)
$$

$$
\begin{aligned}
h_i & =\mathrm{LLM}^-\left(\mathbf{z},\mathbf{w}_{1:i-1}\right), \quad(3)\\
w_i & =\mathrm{argmax}\left(\mathbf{m}_v\cdot h_i\right),	\quad(4)
\end{aligned}
$$

# 4.实现细节

* ***Referring Expression Localization***：这个模块是模型训练的第一阶段，<mark>主要目的是训练视觉*adapter*，让模型能够对齐视觉特征和*LLMs* 的语义空间</mark>。这里文章提到的是使用了其他的数据集来训练（这个数据集包含人工注释的叙述以及鼠标移动轨迹），在训练时，只有一个视觉标记 $$\mathbf{z}=\left\{z_{1}\right\} \in \mathbb{R}^{1 \times d}$$ ，并且面向图像而不是视频，文章认为这可以在空间上将视觉与语言相统一，从而训练适配器，而无需考虑时间信息。为了将相同的语言特征用于定位，只需添加一个*MLP* 作为与词汇映射层平行的定位投影器φ(·)，将语言特征映射到二维位置，公式如下1。该阶段的训练损失如下公式2。

$$
p_i=\varphi(h_i),	\quad(1)
$$

$$
\mathcal{L}_1=\frac{1}{\ell}\sum_{i=1}^\ell\left(\mathrm{CE}\left(\mathrm{LLM}\left(\mathbf{z},\mathbf{w}_{1:i-1}\right),w_i\right)+\lambda\mid\hat{p}_i-p_i\mid\right),	\quad(2)
$$

* ***Pixel-Temporal Alignment*** ：这个模块是为了实现像素级时间对齐，但是也没有使用什么新的对齐方法，而是使用一个信息数据集来训练模型的对齐多模态的能力。具体而言，参考上面提供的模型训练阶段图，在第二个阶段，模型使用新的数据集来训练一个新的*LoRA*，同时保留第一阶段训练得到的*LoRA*。也就是和阶段图保持一致，在该阶段具有两个*LoRA*。主要注意的是，第一阶段已训练的部分，在该阶段保持冻结状态。这一阶段的损失函数如下所示。
  
  $$
  \mathcal{L}_2=\frac{1}{\ell}\sum_{i=1}^\ell\left(\mathrm{CE}\left(\mathrm{LLM}\left(\mathbf{z},\mathbf{w}_{1:i-1}\right),w_i\right)+\frac{\lambda}{P\cdot N}\sum_{j=1}^P\sum_{k=1}^N\mid\hat{p}_{ijk}-p_{ijk}\mid\right),
  $$
* ***Video Question Answering*** ：在第二阶段，文章将高质量的对话数据 *Valley* 和 *Video-ChatGPT* 合二为一，进行指令调整，使模型能够遵循人类指令，从而实现更准确、更普遍的视频理解能力。第三阶段的学习目标采用标准标签平滑交叉熵损失计算，用于生成自动回归，如下公式所示

$$
\mathcal{L}_3=\frac{1}{\ell}\sum_{i=1}^\ell\mathrm{CE}\left(\mathrm{LLM}\left(\mathbf{z},\mathbf{w}_{1:i-1}\right),w_i\right).
$$

# 5.模型性能

实验性能效果对比图：![效果对比图1](https://pic1.imgdb.cn/item/68103b7958cb8da5c8d30e38.png)
![效果对比图2](https://pic1.imgdb.cn/item/68103b9e58cb8da5c8d30e44.png)

# 6.改进/挑战/问题/想法

* **想法**：这篇文章的主要的改进就是1️⃣使用外部数据集单独训练视觉投影层，并且是处理图像的数据集，而不是视频数据集。2️⃣在对齐能力训练上，也是使用了自建的数据集训练模型。**整体而言，这篇文章感觉是将多个训练好的模块集成到一起，完成指定的任务。**
* **效果**：仅关注时间定位这方面的效果，与专门处理时间定位任务的模型相比，这个效果是很差的，可能是大视频语言模型并没有专门关注这个任务，所以这样来看，时间定位效果其实不是很好。
* **问题**：个人感觉比较奇怪的一点是在测试时间定位能力时，选用的是*ActivityNet* 这个数据集，但是这个数据集如果仅用于时间定位是不包含文本描述的，但是这是一个大视频语言模型，所以在测试这个的时候是怎么处理文本的。（本来是想比较一下之前看过的大视频语言模型的效果的）

