---
layout: post
title: 'OmniSTVG: Toward Spatio-Temporal Omni-Object Video Grounding😐'
subtitle: 'OmniSTVG: 面向时空全目标视频定位'
date: 2025-09-29
author: Sun
cover: 'https://pic1.imgdb.cn/item/68d775bac5157e1a883a2fcd.png'
tags: 论文阅读
---

> [OmniSTVG: Toward Spatio-Temporal Omni-Object Video Grounding](https://arxiv.org/abs/2503.10500)

> 💐💐[项目主页 完整代码还未公开](https://github.com/JellyYao3000/OmniSTVG)
> 
> 📌作者单位
> 
> 1. 中国科学院大学
> 2. 北方工业大学
> 3. University of North Texas
> 4. 上海交通大学
> 5. 中国科学院软件研究所

# 1.文章针对痛点

这篇文章关注的是将时空视频定位扩展任务，现有的时空视频定位任务是仅关注单目标的时空定位，但是文章认为这样是**不足以模型理解更多的真实场景，真实场景下不仅仅关注单目标，文本描述往往是关注多目标的；此外，现有的*STVG* 任务还忽视了文本查询目标的局部交互。**

所以文章提出了一个新的任务，也就是将单目标的时空视频定位扩展到多目标的时空视频定位任务。

# 2.主要贡献

针对提出的这个新任务，文章首先定义了新任务，现有的*STVG* 任务是只定位文本描述中的单目标；而文章新提出的任务，是需要定位文本描述提到的所有的目标对象。也就是这个新任务***OmniSTVG* 不仅能够定位任意数量的兴趣目标对象，还能够关注到目标对象之间的局部交互**。

为了促进对*OmniSTVG* 任务的探索，文章构建了一个全新的大规模基准数据集*BOSTVG*。具体地说，*BOSTVG* 包含10,018个视频，其中包括1020万帧，并涵盖了不同场景中的287个类别。*BOSTVG* 中的每个视频都与自由形式的文本查询配对，包含可定位的对象数量的不同，范围从1到10，平均为2.4。并且每个对象都用时空*tube* 手动注释。为了确保高质量，在需要多轮时仔细检查和完善每个序列中的所有管注释。

同时针对新任务，文章也提出了一个简单但是有效的方法*OmniTube*。具体而言，*OmniTube* 基于当前的*Transformer STVG* 方法建立。它包括用于视频和文本融合的多模态编码器以及用于定位的解码器。不同于当前的*STVG* 模型仅定位一个目标，*OmniTube* 同时学习解码器中的多组对象查询，以将视频中的所有对象定位。为了改善定位，文章利用文本指导的视频中的视觉信息来产生查询，从而使学习更好的查询功能受益于目标定位。

原文总结贡献如下：

1. 我们介绍了*OmniSTVG* ，这是一个新的*STVG* 任务，该任务将查询中提到的所有对象都定位，以实现更灵活，更全面的理解；
2. 我们介绍了*BOSTVG* ，这是一个带有10,018个视频的大规模数据集，来自287个类别的*OmniSTVG* 类别中的视频和超过1000万帧；
3. 我们提出了*OmniTube*，这是一种简单但有效的方法，可促进*OmniSTVG* 的未来研究；
4. 我们证明，*OmniTube* 为*OmniSTVG* 实现了有希望的表现，旨在为未来的研究提供参考并为未来的研究提供指导。

数据集对比图：![数据集对比图](https://pic1.imgdb.cn/item/68d77b71c5157e1a883a66dc.png)

# 3.实现流程

文章提出模型的架构图如下所示，有点类似*ICLR2025* 那篇文章的架构。

![模型架构图](https://pic1.imgdb.cn/item/68d77f28c5157e1a883a6abd.png)

# 4.实现细节

文章使用模型的整体架构就是和*ICLR2025* 那篇差不多，针对多目标的任务，唯一的区别就是现有的方法都是使用了一个查询，但是文章是使用了多个查询，这篇文章还是值得参考的，但是*github* 还没上传代码。

# 5.模型性能

新任务的指标还是挺低的。

![模型性能1](https://pic1.imgdb.cn/item/68d9f3fcc5157e1a8842aba3.png)

# 6.改进/挑战/问题/想法

* **想法**：

