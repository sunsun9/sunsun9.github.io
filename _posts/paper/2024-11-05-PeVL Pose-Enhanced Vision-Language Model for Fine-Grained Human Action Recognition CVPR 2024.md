---
layout: post
title: 'PeVL: Pose-Enhanced Vision-Language Model for Fine-Grained Human Action Recognition CVPR 2024'
subtitle: 'PeVL：用于细粒度人体行为识别的姿态增强的视觉-语言模型'
date: 2024-11-05
author: Sun
cover: 'https://pic.imgdb.cn/item/672a07e5d29ded1a8cc7b5d7.png'
tags: 论文阅读
---

> [PeVL：用于细粒度人体行为识别的姿态增强的视觉-语言模型](https://openaccess.thecvf.com/CVPR2024)

> 这篇文章主要是加入了Pose这个模态作为Video和Text模态之间的桥梁，传统的多模态联合方法（或者说大部分多模态模型方法），仅仅联合Video和Text这两个模态。
> 
> 本文在这里加入了Pose模态，并让Pose与Video模态之间进行互相优化。在这之后在加入Text模态，来进一步引导模型学习。我认为这个思路还是可学习的，对于行为识别这个方向来说，本来可结合的模态就比较多，这种思路未尝不可学习。

> 这篇文章,我认为整体的思路很清晰,论文整体行文安排是按照模型架构分区和工作顺序写的,所以本文工作这个模块其实内容很清楚,逻辑性也很好.另一方面的话,在实验部分描述也很清晰详细.
> 
> 但是在本文工作这个模块,我认为有的地方的工作原理其实并没有讲清楚,有些模块并不知道为什么这么设置,或者说这么设置有什么好处.

# 1.摘要

视觉-语言基础模型近期的发展已经揭示了跨模态学习的巨大优势。但是由于在视觉和文本之间存在巨大的鸿沟，因此这些模型可能并没有充分利用跨模态信息的优点。而姿态模态可以在着两者之间建立一个桥梁，从而提高跨模态学习的有效性。

本文提出了一种新的架构*PeVL, Pose-enhanced Vision-language model*，来自适应具有姿态模态的*VL*模型，来学习细粒度人体行为的有效知识。*PeVL*包含两个组件：**非对称跨模态细化模块***(UCMR, Unsymmetrical Cross-Modality Refinement)*和**语义引导的多级对比模块***(SGMC, Semantic-Guided Multi-level Contrastive)***。

* ***UCMR*模块**：包含*Pose-guided Visual Refinement(P2V-R)*和*Visual-enriched Pose Refinement(V2P-R)*。从名字上来看，这个模块是一个视觉和姿态模态二者之间双向的一种互优化。
* ***SGMC*模块**：包含*Multi-level Contrastive Associations of vision-text and pose-text at both action and sub-action levels*（动作级和子动作级的视觉-文本和姿态-文本的多级对比关联）和一个*Semantic-Guided Loss*。可以有效实现文本的对比学习。

# 2.引言

视觉语言模型是通过从大量成对图像-文本数据集中派生出的共享嵌入空间来对齐图像文本特征，其中，原始的图像模态可以提供底层详细的视觉特征，文本模态可以提供高层粗略的语义描述。但是先前的大部分工作都是在*VL*模型上微调，在关联视觉和文本模态之间的巨大差异上还存在限制。而额外的姿态模态可以提供一种身体架构和关节移动的潜在语义表征，能够有效改善视觉和文本之间的*gap*。

所以本文提出了*PeVL*这个新的架构，整体描述和摘要中的差不多，这里就不再赘述。*PeVL*整体架构概览图，左边是传统的*VL*模型，右边是本文提出的模型，其中红色框代表本文新提出的内容。![整体架构概览](https://pic.imgdb.cn/item/672608a6d29ded1a8c9a2e96.png)

# 3.相关工作

本文从两方面阐述了相关工作，分别是行为识别和视觉语言模型的适应。

* **行为识别**：早期使用的是卷积或者序列模型来捕捉时空关系，但随着*vision transformer-based*架构出现，发现该架构明显超越了传统的卷积架构。也有工作是利用人体姿态作为额外模态来联合学习视频。**而本文是直接使用*2D*身体关节坐标作为输入，来学习细粒度动作的结构时间动态；同时利用描述原子动作和动作类别的视频标签和文本提示来监督视频和姿态模态的特征对齐。**
* ***VL*模型的适应**：由于大预言模型的发展，*VL*模型的出现也标志着计算机视觉领域一个重要的转折点，例如，*CLIP*和*ALIGN*的出现。目前的一些工作，通过扩展*CLIP*框架来涵盖视频理解。本文探讨了通过引入额外的姿势模态和带有文本表示的多级监督学习来扩展现有的 *VL*模型，以增强对细粒度动作的理解。

# 4.本文工作

本文提出的*PeVL*模型包含三个部分：（1）三个具有adapters的独立单模态编码器；（2）一个用于有效视频-姿态跨模态学习的**非对称快模态细化模块**，*UCMR*；（3）一个用于动作和子动作级文本引导的*VPL*联合学习的**语义引导多级对比模块**，*SGMC*。架构图如下：![架构图](https://pic.imgdb.cn/item/67274defd29ded1a8c80335b.png)

### 4.1具有*adapters*的单模态编码器

采用的*CLIP*的图像编码器来编码视频和姿态输入，*CLIP*的文本编码器来编码文本输入，在每个编码器之后，加入了一个可训练的*adapters*（具有瓶颈结构）。

> 但是这里为什么要加一个可训练的*adapters*还不是很理解，文章也没有提到为什么要这样做（*adapter*的目的或者作用是什么）。
> 
> 补：根据查阅，个人理解应该这些适配器层通过加入到预训练的编码器某些层之前，可以通过少量参数从而实现模型微调，这样可以减少计算量。这篇文章使用的模型是CLIP，这种大预言模型的参数量一般都很大，如果对整个模型进行微调的话速度可能会很慢。（发现原文说了一句*Our network benefits from large-scale pretrained VL models, initializing with few new parameters for a strong starting point.* 当时还很奇怪，现在发现这句话说的少量参数就是指的adapter的参数）

### 4.2 *UCMR* 模块

视频和姿态是非对称模态，前者包含丰富的底层视频特征，后者包含身体结构的简洁潜在语义表征。为了实现在这两个模态之间的有效跨模态学习，本文设计了这个模块，包含*P2V-R*和*V2P-R*两个部分。

* ***P2V*** ：因为原始的图像具有丰富的视觉信息，但是仅仅针对图像的学习算法可能不能有效关注身体部位，因此，本文提出了一个*Weighted Mask Module* 模块，来引导学习注意力到姿态关节点附近的区域。**（1）**根据*2D* 姿态检测器，得到一组关节位置坐标；**（2）**定义一个*weighted pose mask*，大小就是空间分辨率*H×W*；（这个mask的张量的获取，是通过姿态编码器的最后一层第*(x, y)*  像素的所有关节点权重的归一化；同时，为了对齐*ViT* 输入，将所有的*Pose weighted mask* 进行了分解，其*patch size* 的大小与视频嵌入的*patch size* 大小一致。）**（3）***P2V* 函数作为局部注意力来调节包含权重姿态的视频*token* 表征。
  *P2V* 的注意力权重学习如下：![P2V的注意力权重](https://pic.imgdb.cn/item/6727577ad29ded1a8c867da7.png)
* ***V2P*** ：姿态模态是通过关节点的坐标表示的，这样处理会导致丢失人体视觉上下文信息。因此，在这个模块引入了视觉*tokens*，通过相关的视觉上下文信息来丰富姿态*tokens*。采用**多模态融合方法**。**（1）**对*pose tensor* 进行下采样减少帧的数量，从而匹配视频模态的索引和时间维度；**（2）**连接一个额外的*token* 到姿态嵌入空间中，来表示背景；**（3）**将所有的像素分组到对应的*pose tokens* 中，包括身体关节*tokens* 和一个背景*token*。
  *V2P* 的注意力权重学习如下：![V2P的注意力权重](https://pic.imgdb.cn/item/67275b2ad29ded1a8c89642a.png)
* **细化监督**：上述描述的跨模态学习也可能会引入错误或者干扰信息。例如，在*P2V* 中，姿态估计可能会因为不寻常的身体姿态或者自我遮挡问题而产生错误；在*V2P* 中，丰富的视觉信息可能会引入一些干扰信息。因此，在两个子模块，本文提出了在**模态特征细化前后进行对齐**，从而平衡模态间和模态内的对比学习。具体来说，就是**优化**视频特征和细化后视频特征（姿态特征和细化后姿态特征）之间的**距离**，通过采用归一化温度尺度交叉熵（*NT-Xent*）损失，建立基于余弦距离的对称相似性。优化公式如下:![优化公式](https://pic.imgdb.cn/item/67275db9d29ded1a8c8b3324.png)
* 最后UCMR模块损失函数：![UCMR模块损失函数](https://pic.imgdb.cn/item/67286470d29ded1a8c5a5df3.png)

### 4.3 *SGMC* 模块

本文提出这个模块的目的是，利用详细的文本内容来监督*VPL* 模态之间的学习，并且在全局动作和细粒度子动作级进行对齐。*SGMC* 模块包含两个部分：（1）在动作和子动作级上*VL* 和*PL* 之间相似性的*Multi-level Contrastive Associations*，（2）*Semantic Content Guided Loss*，利用不同细粒度动作文本内容之间的差异来实现弹性对比学习。

* ***Multi-level Contrastive Associations***：根据前面的架构图可以看到，模型将编码的*text representations* 分成了两部分，其中一个是动作级表征*S*，另外一个是子动作级表征*W*。**（1）在处理*VL* 上**，首先将*P2V-R* 模块得到的输出进行处理，得到*clip-level video representations V*；然后在动作级上计算*V* 与*S* 之间对比关联 *S*<sub>*VS*</sub> *= sim(V, S)*。**（2）类似地，在处理*PL*时**，将对比关联分成了动作级和子动作级。在动作级上的计算与*VL* 上的计算类似，*S*<sub>*PS*</sub> *= sim(P, S)*，其中，*P* 的生成与*V *的生成类似。在子动作级上，计算帧级上的姿态表征*F*   与单词级上的文本表征*W* 之间的对比关联矩阵，*Ŝ*<sub>*F W*</sub>  = *F W* <sup>*T*</sup>。最后，![相似度计算](https://pic.imgdb.cn/item/67283b05d29ded1a8c307805.png)
* ***Semantic Content Guided Loss***：本文认为，一个文本标签如果与真实标签有更大的差异，相应地，对应视频之间也会有更大的差异。因此，简单地根据标签文本或动作类别名称视为正样本或负样本可能对于学习细粒度的动作上下文并不准确。因此，本文提出了本模块，该模块使用了一个**强度系数**，通过标签文本之间的**差异强度来调整负样本推动强度**（*pushing strength*)。强度系数通过计算样本文本与真实标签之间的余弦相似度得到，*s*<sub>*i*</sub> = *norm(1 - sim(t*<sub>*i*</sub>, *t*<sub>*g*</sub>))。最后，*video&pose-to-text (vp2t) and text-to-video&pose (t2vp) similarity scores* 计算，![相似度计算2](https://pic.imgdb.cn/item/67283dd6d29ded1a8c33398f.png)
* 这个模块的损失函数：![SGMC模块损失函数](https://pic.imgdb.cn/item/672864c8d29ded1a8c5a9d85.png)

> 模型总的损失函数:![模型总损失](https://pic.imgdb.cn/item/67286521d29ded1a8c5aeacd.png)

# 5.实验部分

实验部分首先描述了实验的基本设置，包括使用的*backbone*、输入数据的形状以及一些权重参数等。其次介绍了本文实验的数据集情况，使用了四个数据集；以及在这些数据集上和*SOTA* 方法的比较结果。

最后，描述了相关组件的消融实验结果。（1）整体组件的消融实验，即+*Pose, +Adapter, +UCMR Block, +SGMC Module*。（2）*UCMR* 内置组件的消融实验。即移除整个*UCMR*、移除*P2V*、移除*V2P* 以及移除监督细化。（3）*SGMC* 内置组件的消融实验，对*Ｓ*<sub>*PS*</sub> 、*Ｓ*<sub>*VS*</sub> 以及*Ｓ*<sub>*FW*</sub> 加入的评估。（4）输入编码器的消融实验。包含*VPL* 模态使用的消融实验、*Adapter* 使用的消融实验以及关节点坐标使用的消融实验（这个主要是指在姿态表示的时候，是使用关节点坐标较好，还是使用热图表示较好）。

