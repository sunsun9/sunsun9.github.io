---
layout: post
title: 'Fewer Steps, Better Performance: Efficient Cross-Modal Clip Trimming for Video  Moment Retrieval Using Language AAAI 2024'
subtitle: '更少步骤，更好性能：面向使用语言视频时刻检索的高效跨模态视频片段修剪'
date: 2025-04-17
author: Sun
cover: 'https://pic1.imgdb.cn/item/67ff71b988c538a9b5d37eaa.png'
tags: 论文阅读
---

> [Fewer Steps, Better Performance: Efficient Cross-Modal Clip Trimming for Video  Moment Retrieval Using Language](https://ojs.aaai.org/index.php/AAAI/article/view/27941)

> ❌❌不提供代码
> 
> 📌作者单位
> 1.湖北省大数据安全工程研究中心，华中科技大学网络空间安全学院
> 2.北京大学
> 3.河南大学
> 4.大连理工大学
> 5.四川大学
> 6.深圳大学
> 7.华中科技大学

# 1.文章针对痛点

这篇文章关注的是使用语言辅助的视频时刻检索任务。

文章认为的主要问题就是目前现有的大部分*VMR* 方法，在处理长的、未剪裁视频时，都采取的要么是采样固定长度的视频片段，要么采样混合长度的视频片段；而文章认为这种方式在随着视频长度增加时，是不太适用的，尤其在面向一些实时应用程序时。所以文章认为需要的VMR模型应该<mark>（1）能够覆盖多场景各种时间长度，（2）能够连接不同模态之间的语义gap，（3）能够理解不同模态的语义细节，从而为最优检索提取建模不变的特征。</mark>（这是文章突出的解决问题的点，文章实现的模型也是主要为了解决这个问题）

另外，文章提出了VMR一些基本的问题。包括需要能建模复杂的阔模态交互，能够捕捉复杂的上下文信息；不仅需要识别物体和动作，<mark>也需要识别什么视觉内容是足够检索准确时刻的</mark>（这个也是文章关注的）。

# 2.主要贡献

文章认为，**不是视频的所有部分都是对推理有用的，并且高层关于物体和动作的视觉语义可以引导注意到检索的地方**。因此，可以学习使用高层语义来缩小任务查询条件先验。

所以针对上面的这些问题，文章基于上面的直觉提出了一种方法。具体来说，使**用便宜的索引特征来回顾视频**，选择一个查询相关的视频片段的小集合，之后仅使用这些视频片段用于目标时刻检索。文章认为这种做法可以有效减少计算花费，但不会牺牲模型性能。

此外，文章进一步设计了三种语义索引特征，来捕获与背景特征、外观特征和动作特征相关的视频内容。

原文总结贡献如下:

1. 在本文中，我们针对*VMR* 采用新颖有效的剪辑选择方法，该方法首先使用廉价的索引功能预览视频，然后选择一小部分与查询相关的夹子，最后仅在最后一刻的时刻检索中使用这些选定的剪辑。
2. 我们通过将三个高级特征（*BAM* 功能）设计为语义索引来提出有效的片段选择模块。然后，自适应剪辑更新策略可以在每次迭代期间以功能蒸馏损失为监督更新选定的剪辑。
3. 我们对三个流行但具有挑战性的基准测试的实验表明，我们的方法比最先进的方法更高效，更有效。

# 3.实现流程

一样，还是先看一下原文展示的模型架构图：![模型架构图](https://pic1.imgdb.cn/item/67ff837e88c538a9b5d3bfa4.png)

这个图稍微感觉有点混乱了。整体的流程就是前面有一个*BAM* 特征提取，后面就是一个递归更新的片段选取过程，经过一定程度的更新后，最后输出特征，经过时刻检索头得到最后的结果。

# 4.实现细节

* ***BAM* 特征提取**：这个过程其实比较简单，就是使用了三种不同的特征提取器来分别提取背景、外观和运动特征。文章采取的做法是每个*clip* 都采样一帧来提取这些*BAM* 特征，记作$s$（也就是所谓的特征索引）。需要特别说明的就是运动特征，文章的做法是使用*clip* 的第一帧和最后一帧来提取运动特征。
* **递归更新选择片段**：1️⃣处理特征索引$s$和*clip* 特征。对于每一个*clip* 特征$c_b$，首先与$s$连接，得到$s_b$；之后，通过跨模态融合得到$c_{b}^{\prime}$，如下公式1；最后根据$c_{b}^{\prime}$按照公式2计算选择概率。最后要么将所有的*clip* 都递归一遍或者达到了最大*step*，就停止更新。整个过程就是结构图种的蓝绿底的部分。

$$
c_{b}^{\prime}=\text { CrossModalFusion }\left(s_{b}, q\right),	\quad(1)
$$

$$
p_{b+1}=\text { ClipSelection }\left(c_{b}^{\prime}\right) \in\{0,1\}^{C},	\quad(2)
$$

* **模型优化**：文章采用了三个损失来实现模型优化，分别是*clip selection loss, VMR task loss, distillation loss*。



# 5.模型性能

实验性能效果对比图：![效果对比图1](https://pic1.imgdb.cn/item/68009b1588c538a9b5d5c4fe.png)
![效果对比图2](https://pic1.imgdb.cn/item/68009b3a88c538a9b5d5c5b5.png)
![效果对比图3](https://pic1.imgdb.cn/item/68009b4f88c538a9b5d5c62e.png)

# 6.改进/挑战/问题/想法

* **想法**：这篇文章的效果还是挺好的，看了一下之前看的华科的开放集的*VMR* 文章，这篇文章在*Charades-STA* 这个数据集上表示要好些，其他相对来说要差一些。这篇文章的出发点我觉得是很好的，就是认为不是所有的片段都是有用的；至于可以覆盖各种长度的视频，其实就是文章将视频分成了固定长度的*clip*，每个*clip* 提取一帧；这个我感觉和均匀分段，每段随机采样一帧。

