---
layout: post
title: 'TE-TAD: Towards Full End-to-End Temporal Action Detection via
Time-Aligned Coordinate Expression CVPR2024'
subtitle: 'TE-TAD: 通过时间对齐坐标表达实现完全的端到端的时序动作检测'
date: 2024-12-02
author: Sun
cover: 'https://pic.imgdb.cn/item/674d4c21d0e0a243d4dbd9d7.png'
tags: 论文阅读
---

> [TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression](https://openaccess.thecvf.com/CVPR2024)
> 1韩国首尔高丽大学人工智能系 2韩国首尔高丽大学脑与认知工程系

# 1.文章针对痛点

还是*TAD* 任务，本文针对的是在*TAD* 任务中，基于查询的检测器在实现完整的端到端建模方面仍然受限。换句话说，应该是目前现有的使用基于查询的检测器方法，大都不是端到端的模型。

文章认为这个限制主要是两方面：

* 基于查询的检测器在解决扩展时间覆盖问题时，会产生性能下降；并且与无锚点的方法相比，它们并不是一个好的选择。原文中提到，现有的基于查询的检测器方法都是采用的是（动作实例的中心位置，宽度）这种坐标表示方式，然后再通过*sigmoid* 函数解码得到开始时间和结束时间。但是原文通过实验验证这种坐标表示方式在处理扩展视频数据（即**动作特征覆盖长度增加**）时，会出现**明显的性能下降和不稳定性上升**；其次，这种表示方式**非常依赖前面提到的动作实例的中心位置**（即会导致模型很敏感），一旦这个数据预测存在微弱的噪声时（即预测与真实值存在微弱的偏差），也会出现明显的性能下降。
* 由于在扩展时间覆盖上受限，对滑动窗口方法的依赖会导致冗余提议并需要使用 *NMS*。（这应该是原文为什么要提出时间坐标对齐表达的原因）

> 扩展时间覆盖：在本问题中应该指的是基于查询的检测器在长时间段的检测上的能力。再联合原文提到的限制，猜测应该是基于查询的检测器在处理长时间数据时，效果不是很好。

同时，文章还提到了依赖滑动窗口方法也有很大的限制。**一方面**由于窗口大小的限制，滑动窗口在解决长时间持续上受限；**另一方面**滑动窗口会有窗口重叠部分，这样会产生重复的预测，往往导致需要使用*NMS* 来去除冗余；这样的做法又与我们想要的最小化手工设计组件的想法矛盾。（这是原文为什么要提出端到端的原因）

# 2.主要贡献

原文认为，规范化坐标表达是解决这个问题的关键。因此，本文提出了*TE-TAD*，这是一个集成了时间对其坐标表达式的完整的端到端时间动作检测*transformer*。具体来说，本文**利用实际时间线数值重新制定坐标表达式**，确保在极其多样化的视频持续时间环境中能实现长度不变的表示。此外，本文还提出了**自适应查询选择**，它可以**根据视频长度动态调整查询数量**，与固定查询集相比，为不同的视频持续时间提供了合适的解决方案。

原文将本文贡献总结为以下几点：

* 原文提出了一种完整的端到端时间动作检测变换器。它集成了时间对齐坐标表达式 (*TE-TAD*)，它保留了集合预测机制，并通过消除手工制作的组件实现了 *TAD* 的完整端到端建模。
* 原文的方法为基于查询的检测器引入了长度不变机制（通过时间对齐坐标表达实现），显著提高了处理不同长度视频的可扩展性。
* 原文认为 *TE-TAD* 明显优于以前的基于查询的检测器，并且与最先进的方法相比具有竞争力，即使没有滑动窗口和 *NMS* 等手工制作的组件（本文设计的模型本身没有使用滑动窗口，但是可以选择是否使用*NMS*，原文通过实验结果验证在没有*NMS* 的情况下，性能也是优于*TadTR* 方法的）。

# 3.实现流程

这篇文章构建的网络模型架构图如下：![网络架构图](https://pic.imgdb.cn/item/674c0cdcd0e0a243d4db956d.png)
采用*TadTR* 作为本文模型的*baseline*，即在*TadTR* 的基础上加了一些组件，从而提高模型的性能。整体的实现过程如下：

* 在向模型输入视频数据后，会使用一个*backbone* （*I3D*）来提取视频特征。再经过处理，得到多尺度特征。
* 多尺度特征输入编码器，经过特殊处理，得到时间对齐的查询，且查询数量不一致。
* 将这些初始的查询经过本文涉及的自适应查询选择，得到一系列的*Top-K Selection*。
* 最后将上一步的查询选择输入解码器中，得到最后的动作提议（原文虽然没有明确提到，但是个人认为应该也是无锚点的方法，即这个生成的提议就是最终的预测结果）。

# 4.实现细节

* **多尺度特征**：在这一步首先将backbone得到的视频特征，输入到一个单层的卷积神经网络中，从而实现将特征对齐到transformer架构的维度，即将特征嵌入到一个新的维度空间中。之后，使用一个步长为2的卷积神经网络，生成每个尺度的特征。公式表达如下：
  $ Z_l=\operatorname{LayerNorm}_l\left(\operatorname{Conv}_l\left(Z_{l-1}\right)\right), l \in\{2, \ldots, L\}. $ 其中$Z_l \in \mathbb{R}^{C \times T_l}$，需要主要的是$T_l$是$T_{l-1}$的一半。
* **时间对齐查询生成**：参照之前的方法，使用参照来和视频真实时间对齐。其中，中心参考$c^{\mathrm{ref}}=\left\{t \times \frac{f}{w \times 2^{l-1}}+\frac{w \times 2^{l-1}}{2}\right\}_{t=1}^{T_l}, l \in\{1,2, \ldots, L\}$，宽度参考$d^{\mathrm{ref}}=\alpha \cdot f \times 2^{l-1}, l \in\{1,2, \ldots, L\}$。根据中心和宽度划分了查询，结合架构图可以看出。（使用它的公式，是可以刚好和之前的多尺度对上的）
* **自适应查询选择**：之前的方法都是固定选择前K个提案，原文认为这种方法存在局限，不能很好的适应视频内容特征。因此，原文提出将视频分成固定长度的$T_{sector}$，并在每个*sector* 中选择前*K* 个。原文认为这种方法可以避免仅仅只选择部分数据。这个过程用公式表示如下：$\mathcal{Q}=\bigcup_{s=1}^S\left\{\left(\hat{c}_{t, l}^{(0)}, \hat{d}_{t, l}^{(0)}\right) \mid(t, l) \in \text { indices of top- } K \text { in } P_s\right\}$，其中$P_S$就是前面提到的每个*sector*。最后得到的$\mathcal{Q}$就是模型选择的查询集合。
* **时间对齐片段细化**：本文没有参考先前的方法使用归一化函数来细化坐标表达，而是使用公式$\hat{c}_q^{(n)}=\hat{c}_q^{(n-1)}+\Delta c_q^{(n)} \cdot \hat{d}_q^{(n-1)}, n \in\left\{1,2, \ldots, L_D\right\}$，$\hat{d}_q^{(n)}=\exp \left(\ln \left(\hat{d}_q^{(n-1)}\right)+\Delta \hat{d}_q^{(n)}\right), n \in\left\{1,2, \ldots, L_D\right\}$实现细化。
* *损失函数*：公式表示为$\mathcal{L}_{\text {total }}(\mathcal{A}, \hat{\mathcal{A}})=\sum_{i=1}^{N_q} \mathcal{L}_{\text {match }}\left(\mathcal{A}_i, \hat{\mathcal{A}}_{\pi(i)}\right)$，其中，$\mathcal{L}_{\text {match}}$是分类损失和回归损失的结合，对于分类损失，本文使用的是焦点损失（*focal loss*）；对于回归损失，本文使用的是*DIoU* 和宽度的对数比距离。此外，还在每个解码器层使用了辅助解码损失。

# 5.模型性能

本文使用了三个数据集，分别是*THUMOS14, ActivityNet v1.3, and EpicKitchens*。选用的评价指标是*mAP*，*THUMOS14* 和 *EpicKitchens* 的 *IoU* 阈值分别设置为 [0.3:0.7:0.1] 和 [0.1:0.5:0.1]，而对于 *ActivityNet v1.3*，结果报告为 *IoU* 阈值 [0.5, 0.75, 0.95]，平均 *mAP* 计算为 [0.5:0.95:0.05]。

在*THUMOS14* 数据集上的性能比较如下，右上角的符号表示使用了不同的*backbone*，没有标记的使用的*I3D*，其他的标记可以阅读原文了解。从图中可以看到本文提出来的模型在基于查询的方法中是达到了*SOTA* 的性能，但是与无锚点*SOTA* 方法*TriDet* 相比，还是稍微逊色。![THUMOS14数据集上的性能比较](https://pic.imgdb.cn/item/674d4598d0e0a243d4dbd8c7.png)

在*ActivityNet v1.3* 数据集上的性能比较如下，整体上性能效果比较结果与*THUMOS14* 数据集上的比较结果差不多。![ActivityNet v1.3数据集上的性能比较](https://pic.imgdb.cn/item/674d469cd0e0a243d4dbd8ef.png)

在*EpicKitchens* 数据集上的性能比较如下，整体上性能效果比较结果与*THUMOS14* 数据集上的比较结果差不多。![EpicKitchens数据集上的性能比较](https://pic.imgdb.cn/item/674d473ad0e0a243d4dbd909.png)

# 6.改进/挑战/问题

* **改进**：原文提到，通过解码器的自注意力解决整个查询对于保留集合预测机制和完整的端到端建模至关重要。感觉也可以在自注意力上进行改进或者什么操作，应该也可以提升性能。目前大语言模型结合计算机视觉的方法很多，这个任务应该也可以结合文本模态试试。
* **挑战**：根据本文的实验结果，目前的无锚点方法是十分依赖*NMS* 后处理方法的（从实验结果中可以看到在不使用*NMS* 的情况下，平均*mAP* 下降了24.4）。认为可以考虑一下使用怎么方法或者怎么设计可以减轻无锚点方法对*NMS* 的依赖。
* **问题**：（1）针对上面的*NMS*，本文这个是怎么降低对*NMS*　依赖的，感觉目前还没有很懂，不知道怎么实现的一个动作实例只生成一个预测结果。个人理解的是模型在不同尺度上进行预测和分类，那最后产生的结果应该也是不同尺度上的结果，这样的话也是有多个结果，应该是需要整合得到最后一个结果的，但是原文并没有提到。

