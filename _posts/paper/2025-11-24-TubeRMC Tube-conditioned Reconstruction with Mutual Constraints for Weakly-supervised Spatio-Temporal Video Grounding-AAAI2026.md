---
layout: post
title: 'TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding AAAI2026🙁'
subtitle: 'TubeRMC: 面向弱监督时空视频定位的具有相互约束的Tube条件重建'
date: 2025-11-24
author: Sun
cover: 'https://pic1.imgdb.cn/item/6923f8573203f7be002a1a95.png'
tags: 论文阅读
---

> [TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding](https://arxiv.org/abs/2511.10241)

> ❌❌[未开源]
>
> 📌作者单位
>
> 1. 中山大学
> 2. 广东信息安全科技重点实验室
> 3. 中国机器智能与先进计算教育部重点实验室
> 4. 香港科技大学
> 5. 华中科技大学

# 1.文章针对痛点

文章首先提出全监督时空视频定位任务，严重依赖大量细粒度的注释信息。其次，现有的弱监督方法都是**遵循简单的后融合范式**，也就是先使用一个预训练的检测器来生成目标对象的*proposals*，之后再和输入文本查询匹配；这种范式导致生成的管道**是独立于文本描述的，容易产生错误的目标对象识别**。

因此，文章提出可以使用预训练的视觉定位模型。一种简单的方法是利用这些模型逐帧生成定位结果，从而获取时空管道。但是，文章认为这对弱监督时空视频定位是低效的。因为**目标识别在跨帧上是不一致的，且时间边界经常不能成功捕捉到目标事件**。总体而言，这种做法模型缺乏时空理解。

# 2.主要贡献

为了更好的利用视觉定位模型，文章提出了一种**Tube条件重建**来增强*tube-text* 对应学习和事件理解，利用时空相互限制来细化*bounding box* 和时间边界。具体而言：

* **Tube条件重建**，是探究视觉线索和文本描述之间丰富的语义依赖，来重建*tube* 上的掩码文本条件。
* 针对上述的条件重新，文章从时间、空间和时空方面设计了三种重建策略。通过从文本中重建关键短语来实现的，在1D、2D 和 3D 高斯注意力图指导下生成的管特征为条件；每种策略都采用*tube* 条件重建器，它利用*tube* 特征从视觉表示中重建屏蔽短语。
* 为了进一步提高管建议的质量，文章也引入了相互约束，包括：确保同一场景内对象的运动连续性的时间到空间约束，以及强制时间边界与高置信度帧对齐的空间到时间约束。



原文总结贡献如下：

1. 我们提出了一种新颖的 *TubeRMC* 框架来学习细粒度的管文本对齐，而不需要边界框或时间间隔的注释。
2. 我们提出管条件重建，从1D、2D、3D（时间、空间、时空）角度全面捕获*tube-text* 对应关系，并相互约束以提高建议质量。
3. 我们的方法在*VidSTG* 和*HCSTVG* 基准测试中优于以前最先进的方法。

# 3.实现流程

文章设计的模型架构首先采用预训练的视觉定位模型来提取每帧的图像文本对应关系和空间定位结果，然后执行跨帧建模以捕获时空上下文并生成不同的重建建议。

为了在没有空间或时间注释的情况下学习细粒度的管文本对齐，文章提出了管条件重建学习。该学习方案主要结合了三种新颖的重建策略，分别从时间、空间和时空角度重建屏蔽短语，全面捕获视觉线索和查询描述之间的时空对应关系。此外，文章引入相互约束来提高重建任务的提案质量。

架构图如下：

![架构图](https://pic1.imgdb.cn/item/6923fe423203f7be002a506c.png)

# 4.实现细节

#### **跨模态提取**

​	首先文章利用预训练的视觉定位模型，逐帧进行定位；针对每帧，选择分数最高的一个框作为当前帧最终定位框。其次，由于第一步的做法是单帧独立进行的，因此在该步要进行时序建模；文章的做法是将第一步生成的跨模态特征送入*TimeFormer* 中。最后，第二步的特征用于进行空间定位细化和时间候选提案生成；空间细化是有一个聚合头完成细化，时间候选提案生成是有一个解码器完成。**上述是基本的流程，但是文章也提到设计了一个时空解码器**。

#### **Tube条件重建学习**

​	为了在*Tube* 条件重建中实现梯度反向传播，文章将时间间隔、空间框和时空管转换为高斯分布。对于时间提议生成器中的时间间隔，使用一维高斯函数来获取时间注意掩码。对于 *Spatial Boxes Refiner* 中的空间框，对每帧应用 2D 高斯函数，将中心坐标视为平均值，将宽度视为标准差，以产生空间注意掩码。对于时空管，采用3D高斯函数，以管中心坐标为分布中心，以管的长、宽、高为标准差，制定尺寸为时空注意掩码。

​	为了捕获视觉线索和语言描述之间全面的时空对应关系，文章构建了三种类型的重建策略，包括时间重建、空间重建和时空重建。**对于时间重建**，谓词及其直接相关的名词被选择性地屏蔽，其中主要包括运动感知时间信息。然后，分别转换为一维高斯时间注意力掩模。对于空间重建，掩盖主语名词以及其形容词，并使用 2D 空间高斯掩码来重建屏蔽文本。这有助于模型专注于视频和文本中对象外观信息之间的对应关系。对于时空重建，利用 3D 高斯掩码进行 TR 并随机掩码跨句子的一组单词，掩码动词、名词和形容词的概率更高。这使得模型能够有效地捕获时空耦合对应关系。



![设计架构](https://pic1.imgdb.cn/item/692404ca3203f7be002a892e.png)

#### **相互限制学习**

​	为了提高重建的对象一致性和时间预测质量，文章提出了相互约束学习。**空间到时间约束利用空间置信度分数来指导生成更精确的时间建议**。首先使用来自 Spatial Boxes Refiner 的分数来生成积极的建议。具体来说，通过选择前 K 个评分帧作为具有预定义宽度的时间中点来生成初始建议。然后，提案生成器预测中点和宽度偏移，将其应用于产生最终的正提案。类似地，使用 K 最低分数的帧构建负时间提议。在此基础上，定义了时空约束损失，以最小化不同正提议之间以及正负提议之间的重叠。

​	此外，提出了时间到空间约束，以确保对象在每个场景中的相邻帧之间保持空间连续性。这是通过应用一个损失来实现的，该损失对每个时间提议中 IoU 低于阈值的相邻帧中的预测框进行惩罚。



# 5.模型性能

![性能对比图](https://pic1.imgdb.cn/item/6924070c3203f7be002a92bc.png)

# 6.改进/挑战/问题/想法

* 参考了解