---
layout: post
title: 'Synergy of Sight and Semantics: Visual Intention  Understanding with CLIP ECCV 2024'
subtitle: '视觉和语义的协同：利用CLIP实现视觉意图理解'
date: 2025-03-29
author: Sun
cover: 'https://pic1.imgdb.cn/item/67e77b3b0ba3d5a1d7e5de66.png'
tags: 论文阅读
---

> [Synergy of Sight and Semantics: Visual Intention  Understanding with CLIP](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01721.pdf)

> 💐💐提供代码
> 
> 📌作者单位
> 1.武汉大学计算机学院多媒体软件国家工程研究中心
> 2.新加坡南洋理工大学

# 1.文章针对痛点

这篇文章关注的是**图像的多标签意图理解任务**，多标签意图理解任务就是不仅仅关注视觉内容，需要挖掘视觉背后的隐含信息，比如理解上下文、文化意义以及情感共鸣等等。

但是现有的方法大多关注的是一些可观察的组件，比如形状、颜色、空间顺序等等，而这并不能有效解决语义理解问题。

文章还提到在训练阶段允许网络完全适应（应该就是指全训练）会破坏模型（面向视觉）预训练的得到的知识。（这一点感觉文章提到的比较突兀，可能是为了支持不全训练的观点吧，也就是模型使用*CLIP* 微调的方式）

# 2.主要贡献

所以针对上面的问题，文章引入了一种新的架构，**称为*Intention Understanding with  CLIP (IntCLIP)***。*IntCLIP* 旨在利用视觉知识，同时确定语义意图线索。它具有**双分支架构**：一个分支维护原始*CLIP* 的不可变图像编码参数，而另一个具有语义适应性的分支则在训练期间进化以专注于意图线索。文章认为这种配置有助于捕获视觉和语义信息，从而产生全面的图像特征表示。（也就是*CLIP* 的两个分支，训练图像分支，对文本分支的*prompt* 进行了处理）

此外，文章还引入分层类别集成以有效利用多层标签，并提出了视觉辅助聚合方法增强了视觉特征地图，帮助识别意图理解的关键区域。

原文总结贡献如下:

1. 我们是第一个将CLIP的视觉知识适应*MIU* 领域的，显着增强了其推理能力。
2. 我们引入了*IntCLIP* 框架、分层类集成和视觉辅助聚合，以有效地促进这种适应。他们的协同作用对于指导意图线索的提取和确定关键意图相关领域至关重要。
3. 我们的方法通过*MIU* 基准测试和图像情感识别（*IER* ）等其他主观任务的实验得到了经验验证，证明了比现有方法的显着优势。

# 3.实现流程

一样，还是先看一下原文展示的模型架构图：![模型架构图](https://pic1.imgdb.cn/item/67e77e2e0ba3d5a1d7e5dea7.png)

整体就是参考了*CLIP* 的架构，分为一个图像编码分支和一个文本编码分支，最后使用对比学习。也就是在CLIP的基础上增加了一些组件，感觉文章可能也是追热点，在*CLIP* 基础上小调一下就发了。

# 4.实现细节

* **文本分支** ：文本整体来说就是使用冻结的*CLIP* 文本分支，但是输入的*prompts* 不同于原本的*CLIP*。文章引入了分层类别集成处理标签，也就是实现了一个下图展示的处理过程；就是使用大语言模型处理原来的意图标签，进行了一定的信息过滤和整合，最后形成一个固定长度的标签，过程如下公式所示。后面的处理就是仿照*CLIP* 的文本分支，这里也没有进行微调训练![分层类别集成](https://pic1.imgdb.cn/item/67e784440ba3d5a1d7e5e048.png)

$$
\mathrm{Prompt}^{+}=
\begin{bmatrix}
V_1^+,V_2^+,\ldots,V_{N^+}^+,\mathrm{CLS}
\end{bmatrix}, \\
\mathrm{Prompt}^{-}=
\begin{bmatrix}
V_1^-,V_2^-,\ldots,V_{N^-}^-,\mathrm{CLS}
\end{bmatrix},
$$

* **图像分支**：在前面使用了一个双图像编码器框架增强了 *CLIP* 架构，也就是架构图右边展示的部分，左边的视觉编码器分支在训练阶段保持冻结状态，右边的语义编码器在训练阶段部分训练。通过这个网络部分得到视觉特征和语义特征后，会经过一个**视觉辅助的融合模块**。该模块分为两部分，分别是**语义查询注意和视觉辅助建模**。<mark>语义查询注意</mark>首先对前面获得的语义特征经过两个卷积获取两个不同的特征子图，如下公式1所示；之后使用这两个子图分别作为查询和键，构建注意力图，如下公式2。<mark>视觉辅助建模</mark>与语义查询注意模块一样，先对视觉特征使用卷积操作，得到$$M_v^{\prime}(i)$$；在根据公式3得到最后的输出。文章针对这部分架构图如下。![视觉辅助的融合模块](https://pic1.imgdb.cn/item/67e7c8340ba3d5a1d7e66b58.png)

$$
M_{q}=Q_{conv}(M_{sem}),M_{k}=K_{conv}(M_{sem}),	\quad(1)
$$

$$
M_s(i,j)=\frac{\exp(M_q^{\prime}(i)\cdot M_k^{\prime}(j))}{\sum_{i=1}^N\exp(M_q^{\prime}(i)\cdot M_k^{\prime}(j))}	\quad(2)
$$

$$
M_i(j)=\alpha\sum_{i=1}^N(M_s(j,i)\cdot M_v^{\prime}(i))+M_{sig}(j), \quad(3)
$$


# 5.模型性能

实验性能效果对比图。![效果对比图1](https://pic1.imgdb.cn/item/67e7c8bd0ba3d5a1d7e66b84.png)

# 6.改进/挑战/问题/想法

* **想法**：本来以为这个是可以与视频理解相关的内容的，但是这个是多标签任务，目前的话，视频理解我不是太清楚，但是应该还没到图像的这种多标签理解上。但是这篇文章也就是将*CLIP* 迁移到了一个新任务，并且针对这个新的任务做了微调，并没有什么很大的创新。

