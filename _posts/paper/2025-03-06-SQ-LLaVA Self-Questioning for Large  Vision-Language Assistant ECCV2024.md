---
layout: post
title: 'SQ-LLaVA: Self-Questioning for Large  Vision-Language Assistant ECCV2024'
subtitle: 'SQ-LLaVA：用于大视觉语言助手的自问答'
date: 2025-03-06
author: Sun
cover: 'https://pic1.imgdb.cn/item/67c68f41d0e0a243d40b4196.png'
tags: 论文阅读
---

> [SQ-LLaVA: Self-Questioning for Large  Vision-Language Assistant](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01393.pdf)
> 
> 1 美国纽约州罗切斯特理工学院；
> 2 销售大通AI研究，美国加利福尼亚州，美国

# 1.文章针对痛点

这篇文章关注的是大视觉语言模型。

虽然最近大视觉语言模型已经取得了一定的进展，1️⃣但是目前**建立预训练视觉编码器与大语言模型之间鸿沟的桥梁，成为了整个网络的瓶颈**; 并且两者之间的**鸿沟会限制二者的泛化能力和特征表示能力**.

而为了解决二者之间的鸿沟, 现有的解决方法大致可以分为三类, 分别是构建一个更鲁棒的图像特征提取器, 收集更高质量的训练数据, 或在预训练阶段同时全微调视觉和语言模型. 2️⃣**但是上述的这些方法无疑会带来更高的计算花费和更多的数据收集, 甚至可能需要富有经验的人为手工设计和大量的数据注释.** 3️⃣其次, 也不能彻底探索包含在图像中的丰富上下文信息。

# 2.主要贡献

为了解决上面的问题，文章提出了一个新的视觉自问自答方法, 通过训练大预言模型回答问题，并在没有收集来自其他源的额外数据的情况下发现视觉线索。下图展示了传统的问答与文章提出的自问自答的对比。![对比](https://pic1.imgdb.cn/item/67c69c6bd0e0a243d40b52d4.png)

文章引入的自问自答*LLaVA*， 简称*SQ-LLaVA*。这个方式是完全利用指导数据中的问题作为额外的可学习源，来训练*LLMs* 并赋予模型好奇心。

为了有效地调整视觉和语言，文章**应用*LoRAs* 来优化*SQ-LLaVA*中的视觉编码器和指导的*LLM***。另外，文章**开发一个原型提取器来增强视觉表示**，通过利用有意义语义信息的可学习*clusters*，来进一步提高视觉和语言对齐方式。

文章总结的工作如下：

1. 我们通过利用教学数据中的高度相关问题上下文来提出一种新颖的培训技术，即视觉语言助手（*SQ-LLaVA*）的视觉自我提出。这项*SQ* 学习任务促进了教学*LLM*，以了解图像和问题之间的关系，增强视觉语言对齐，而无需新的数据收集。
2. 我们为*SQ-LLaVA* 设计并开发了一个轻巧的调整体系结构，由*ViT-LoRA*，*LLM-LoRA* 和原型提取器组成。原型提取器可增强视觉嵌入，而*ViT-LoRA* 和*LLM-LoRA* 在训练过程中有效地对齐视力和语言领域。
3. 广泛的实验结果表明，所提出的*SQ-LLaVA* 可以在多个任务中提高性能更好，包括视觉提问，视觉指导基准和零击图像字幕。

# 3.实现流程

一样，还是先看一下原文展示的模型架构图：![模型架构图](https://pic1.imgdb.cn/item/67c69ef0d0e0a243d40b571a.png)

从架构图中可以看到整个架构包含四个部分，分别是：

* 预训练的视觉编码器。文章使用的是*CLIP-ViT*，负责提取输入图像的*token* 的序列嵌入空间；
* 一个原型提取器。该部分是学习视觉簇，增强原始的图像*token*；
* 一个可训练的具有两个线形层的投影层。负责解决视觉和文本维度不对齐的问题；
* 文章提出的*LLM backbone*。这部分是由预训练的*Vicuna* 实现的，来预测前一个嵌入序列的下一个*token*。

整体的过程可以用以下公式表示，也就是在给定图像*token* $$H_{v}$$ 和 问题*token* 嵌入$$H_{q}$$ 以及前面所有的回答*token* 嵌入$$H_{a}^{(1: i)}$$，得到在位置$${i+1}$$预测回答*token* $$H_{a}^{(i+1)}$$ 的概率。

$$
p_{\theta}\left(H_{a}^{(i+1)} \mid H_{v}, H_{q}, H_{a}^{(1: i)}\right)=\sigma\left(f\left(H_{v}, H_{q}, H_{a}^{(1: i)}\right)\right),
$$

# 4.实现细节

* **视觉自问构建** ：总体来说就是问题会包含比答案更多的信息，相对于回答问题，提出问题需要有更强大的理解能力。在这一部分文章设计了一些提示。具体可以参考下图，其中图说明中的*system-message* 是作为固定提示添加在每一个指令数据的前面，而图中显示的[usr]和[vusr]分别表示给出答案和提出问题，也就是说如果是[usr]，模型只需要给出问题的答案即可，使用的问题由数据集提供；后面的$$<o^{d}>$$ 标记结束位置。![提示](https://pic1.imgdb.cn/item/67c6a96bd0e0a243d40b756d.png)
  也就是整个生成过程可以用下面公式表示，$$R$$ 表示一个随机数，用于控制自问自答对的比例。

$$
\left.X_c^j=\left\{
\begin{array}
{lc}([\mathrm{usr}],X_q^{(j)},\mathrm{[aswr}],X_a^{(j)}) & j=1\mathrm{~or~}j>1,R<\delta \\
(\mathrm{[vusr}],X_q^{(j)},\mathrm{[aswr}],X_a^{(j)}) & j>1,R>\delta
\end{array}\right.\right.,
$$

* **增强的视觉表征**：这个模块是为了更好的进行视觉自问自答，文章设计了一个**原型提取器**，来识别和聚类来自潜在空间的视觉信息的相似模式，目的就是通过原型学习增强视觉表征。原型提取器包含两部分，分别是簇中心优化和原型信息分布，文章采用随机初始化256个簇中心，并采用迭代期望最大化聚类过程（这个聚类过程其实没太懂）。

# 5.模型性能

实验性能效果对比图：![效果对比图](https://pic1.imgdb.cn/item/67c80547d0e0a243d40c8821.png)

# 6.改进/挑战/问题/想法

* **想法**：这篇文章是针对图像的这种视觉语言模型，这篇文章的创新点主要集中在提出了要能生成好的问题，并且也不是说完全是模型生成问题的这种全自问自答形式，而是自问自答结合问答形式。此外，提出了一个聚类的形式帮助理解上下文。

