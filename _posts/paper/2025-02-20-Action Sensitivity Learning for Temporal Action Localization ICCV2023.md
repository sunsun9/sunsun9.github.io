---
layout: post
title: 'Action Sensitivity Learning for Temporal Action Localization ICCV2023'
subtitle: '用于时间动作定位的动作敏感性学习'
date: 2025-02-20
author: Sun
cover: 'https://pic1.imgdb.cn/item/67b5676ed0e0a243d400ae0b.png'
tags: 论文阅读
---

> [Action Sensitivity Learning for Temporal Action Localization](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Action_Sensitivity_Learning_for_Temporal_Action_Localization_ICCV_2023_paper.pdf)
> 
> 1 ReLER Lab, CCAI, 浙江大学；
> 2 阿里巴巴

# 1.文章针对痛点

这篇文章关注的是时间动作定位，和前面的文章很多提到的一样，仍然任务就是检测视频中出现的动作实例的类别和出现的开始&结束时间，这个任务只是有很多名字，但是都是这个任务。

这篇文章认为现有的大部分方法都是直接预测动作类别，回归偏差到边界，这样的方法**忽视了每帧的差异重要性**。也就是文章认为每帧重要程度是不一样的，就算都是包含动作的帧，但是这些帧的重要性是不同的。

# 2.主要贡献

而为了解决上面的问题，文章首先引入了一个概念，即**动作敏感性**，来测量帧的重要性。动作敏感性也**分为两个部分**，分别是用于**分类子任务的动作敏感性**和用于**定位子任务的动作敏感性**。对于每个子任务来说，每帧拥有更高的动作敏感性，那对于这个子任务就越重要，进而在训练时，就会更关注这些帧。

基于上面的这个概念，文章为每个子任务提出了一个**轻量的动作敏感性评估者（*ASE* ）**，能够更好的利用帧级的信息。具体来说，对于一个具体的子任务，*ASE* 从**两个方面学习动作每帧的动作敏感性，分别是类级别和实例级别**。**类级别观点是**为每个动作类别建模粗糙的动作敏感性分布，并通过结合高斯权重实现；**实例级别的观点**是补充类级别建模，并以预测感知模式监督。之后每帧的训练权重会根据动作敏感性动态调整。

文章又基于上面提出的*ASE*，提出了文章的**架构模型动作敏感性学习架构*ASL***。同时，为了进一步增强特征，并提高动作和背景之间的判别行，文章设计了一个新的**动作敏感性对比损失*ASCL***；它精心生成各种动作相关和动作不相关的特征，并在他们之间进行对比学习。

原文总结本文贡献如下：

* 提出了一个具有动作敏感性评估器组件的新型框架，以通过发现对特定子任务的动作敏感框架来提高训练，该框架是从类别和实例级别建模的。
* 设计了动作敏感的对比损失，以提高功能并增加动作和背景之间的可区分性。
* 在多种类型的各种动作定位数据集上验证*ASL*：i）密集标记（即*MultiThumos and Charades*）。 ii）自我中心视角（*Ego4d-Moment Queries v1.0  and Epic-Kitchens 100* ）。 iii）几乎单个标签（*Thumos14* 和*ActivityNet1.3* ），并取得了较高的结果。

# 3.实现流程

一样，还是先看一下原文展示的模型架构图：![模型架构图](https://pic1.imgdb.cn/item/67b6b1d2d0e0a243d400e61f.png)

1. 这个工作流程首先是使用*3D CNN* **获取视频特征**，然后使用*transformer* 编码器进行**特征编码**；
2. 第二步使用*ground truth location* 进行**采样**，得到所有的*ground truth* 片段（这个文章并没有详细介绍，只是在上述的图注释中提到了，猜测应该是提取视频片段所有的*ground truth* 动作片段）；
3. 第三步就是经过文章提到的**动作敏感性评估器**，这里按照前面的设想，分成了两个分支，分别是类级别和实例级别；
4. 最后一步就是**分别经过定位头和分类头**，在这一步还提到了**动作敏感性对比学习**；目的其实还是增强特征，因为一个视频中可能不止包含一个动作实例，但是在提取的时候，模型都进行了提取，所以可以通过这个动作敏感性对比学习，**让相同类别的敏感性帧更靠近，而让不同类别的帧远离，同时也远离背景帧**。

# 4.实现细节

* **特征编码**：*ASL* **利用*transformer* 编码器和特征金字塔将特征序列编码为多尺度表示**。同时为了增强特征，文章在*transformer* 编码器中设计了一种**新的注意力机制**，即并行操作时间注意力和通道注意力，然后融合两个输出。其中，新注意力机制中的时间注意力计算如下公式1，通道注意力表示如公式2，最后二者融合如公式3。

$$
f_{\mathrm{ta}}^{^{\prime}}=\mathrm{softmax}(\frac{Q_{t}K_{t}^{T}}{\sqrt{D}})V_{t}  \quad(1)
$$

$$
f_{\mathrm{ca}}^{^{\prime}}=\mathrm{softmax}(\frac{Q_dK_d^T}{\sqrt T})V_d	\quad(2)
$$

$$
f^{^{\prime}}=(1-\theta)f_{\mathrm{ta}}^{^{\prime}}+\theta f_{\mathrm{ca}}^{^{\prime}T}	\quad(3)
$$

* **动作敏感性评估器**：这个主要考虑到不是所有的帧都是对子任务有贡献，并且贡献程度也不一样，所以想先评估一下每帧对两个子任务的贡献程度。
  ***（1）分解为两个层级*** ：文章认为一个特定类别的动作有着相似的模式，但是在不同场景或者不同的动作主体时，会有细微的差别；这就会导致子动作各自的持续时间以及时间占比比较依赖动作实例的场景和上下文。因此，文章将**每帧的动作敏感性分为了类级别和实例级别**。
  ***（2）分解为两个子任务*** ：文章提到分类动作敏感性需要建模，因为分类敏感性帧不容易定义；实际上，定位动作敏感性也需要建模。其实，我认为就是是分类敏感性和定位敏感性所认定的帧不一定是完全一样的，所以需要分成两个子任务分别进行。
  ***（3）类级别建模*** ：文章认为类级别敏感性具有一定的**先验知识**，<mark>也就是视频帧通常是连续的，并且在所有帧中经常会存在一些关键帧会让敏感性具有一个峰值。</mark>在这种情况下，文章采取整合具有可学习参数的类高斯权重来建模类级别动作敏感性；对于**分类子任务**，其建模的动作敏感性如下公式1（$$\mu$$ 和 $$\sigma$$是可学习参数，$$d(i)$$是当前第$$i$$帧到*ground-truth* 片段中心的距离）；对于定位子任务，因为包含开始和结束时间，所以这部分的动作敏感性建模包含两部分，如公式2所示。

$$
p_i^{cls}=\exp\{-\frac{(d(i)-\mu_c)^2}{2\sigma_c^2}\}	\quad(1)
$$

$$
p_{i}^{loc}=\underbrace{\exp\{-\frac{(d(i)-\mu_{c,1})^{2}}{2\sigma_{c,1}}\}}_{p_{i}^{sot}}+\underbrace{\exp\{-\frac{(d(i)-\mu_{c,2})^{2}}{2\sigma_{c,2}}\}}_{p_{i}^{eot}}	\quad(2)
$$

***（4）实例级别建模*** ：文章认为在上一步的建模中，不能发现所有的敏感性帧，因此需要实例级别建模来补充，帮助捕获类级别建模没有发现的敏感性帧。实例级别评估器直接在每帧上操作，其评估器包含一个*1D* 的时间卷积网络、一个全连接层和一个*sigmoid* 激活函数；这部分的建模如下公式1，其中，$$\Phi^{cls}$$ 和 $$\Phi^{loc}$$ 表示两个子任务分别对应的评估器。

其次，文章认为实例级别建模很难以一种无监督的模式学习，所以这部分是在有监督的模式下学习的。所以这一部分的损失表示如下公式2，上面带上划线的都表示*ground-truth*。

$$
\begin{cases}
q_i^{cls}=\Phi^{cls}(f_i) \\
q_i^{loc}=\Phi^{loc}(f_i) & 
\end{cases}	\quad(1)
$$

$$
\begin{cases}
\bar{Q}_i^{cls}=\varphi(\mathrm{s}_i[\bar{c})]) \\
\bar{Q}_i^{loc}=\mathrm{tIoU}(\Delta_i,\bar{\Delta}) & 
\end{cases}	\quad(2)
$$

$$
\mathcal{L}_s=\mathrm{MSE}(q^{cls},\bar{Q}^{cls})	\quad(2)
$$

最后整个这一部分的结果表示如下公式1，损失如下公式2。

$$
\begin{cases}
h^{cls}(\bar{c})=p^{cls}\mathbb{1}[\bar{c}]+q^{cls} \\
h^{loc}(\bar{c})=p^{loc}\mathbb{1}[\bar{c}]+q^{loc} & 
\end{cases}	\quad(1)
$$

$$
\mathcal{L}_{cls}=\frac{1}{N_{pos}}\sum_i(\mathbb{1}_{in_i}h_i^{cls}(\bar{c}_i)\mathcal{L}_{\mathrm{focal}_i}+\mathbb{1}_{bg_i}\mathcal{L}_{\mathrm{focal}_i})	\\
\mathcal{L}_{loc}=\frac{1}{N_{pos}}\sum_{i}(1_{in_{i}}h_{i}^{loc}(\bar{c}_{i})\mathcal{L}_{\mathrm{DIoU}_{i}})	\quad(2)
$$

* **动作敏感性对比学习**：在深入研究过程中，文章认为**存在三个缺点**，i）分类敏感和定位敏感帧完全不同，导致这两个子任务的不对齐。 ii）不同类别动作中的特征不能区分。 iii）动作内特征和边界外特征没有太大区别。因此，文章提出了**动作敏感性对比学习**，因为与类别相关，所以考虑的是类别级特征；通过公式1得到特定类别的相关帧特征；因为同时需要与背景帧区分开，背景帧获取如公式2；最后对比学习公式如公式3，正样本为同一类别的$$f_{c l s}$$ 和 $$ f_{l o c}$$，负样本为不同类别的$$f_{c l s}$$ 和 $$ f_{l o c}$$以及背景帧$$ f_{bg}$$。
  $$
  \left\{\begin{array}{l}f_{c l s}=\frac{1}{T} \sum_{t} p_{t}^{c l s} \mathbb{1}[\bar{c}] f_{t}, t \in T_{c l s} \\
  f_{l o c}=\frac{1}{T} \sum_{t} p_{t}^{l o c} \mathbb{1}[\bar{c}] f_{t}, t \in T_{l o c}\end{array}\right.	\quad(1)
  $$

$$
f_{bg}=\frac{1}{T}\sum_{t}f_{t},t\in[\bar{t}^{s}-\delta N_{f},\bar{t}^{s}]\cup[\bar{t}^{e},\bar{t}^{e}+\delta N_{f}]	\quad(2)
$$

$$
\mathcal{L}_{\mathrm{ASCL}}=\frac{1}{N}\sum_{B}-\operatorname{log}\frac{\sum_{f_{x}\in\mathcal{P}_{f_{*}}}\operatorname{sim}(f_{*},f_{x})}{\sum_{f_{x}\in\mathcal{P}_{f_{*}}}\operatorname{sim}(f_{*},f_{x})+\sum_{f_{x}\in\mathcal{N}_{f_{*}}}\operatorname{sim}(f_{*},f_{x})}	\quad(3)
$$

# 5.模型性能

主要就是看看这个方法在两个数据集上的效果：![数据集](https://pic1.imgdb.cn/item/67b6deafd0e0a243d400f774.png)

# 6.改进/挑战/问题/想法

* **想法**：关于这篇文章我觉得它的主要想法就是有全局的衡量，也就是类级别；也就帧的衡量，也就是实例级别。其次，就是主要的思路总结下来感觉就是，判断哪些帧是重要的，以及这些帧虽然重要，但是有多重要，怎么去衡量这个重要程度。另外，这篇文章结合了对比学习，也是一个很重要的思路。

