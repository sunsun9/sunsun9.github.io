---
layout: post
title: 'CLIP-Adapter: Better Vision-Language Models with Feature Adapters IJCV2024'
subtitle: 'CLIP-Adapter：具有特征适配器的更好的视觉语言模型'
date: 2024-11-10
author: Sun
cover: 'https://pic.imgdb.cn/item/67307caad29ded1a8c5fbe66.png'
tags: 论文学习
---

> [CLIP-Adapter: Better Vision-Language Models with Feature Adapters International Journal of Computer Vision](https://link.springer.com/article/10.1007/s11263-023-01891-x)
> 
> 整体来看,文章思路还是比较简单的,这个文章原文用了大量篇幅验证模型效果以及消融实验效果.这个想法其实比较简单,原文说明实验全部都是在单卡A100跑完的.所以说明计算量确实不是很大.目前很多论文都是基于大模型微调,所以就看怎么微调实现的效果比较好.(这篇文章的思路也是来源于其他文章的,有一篇文章是将适配器使用到了模型的每一层)



# 前言

大规模对比视觉语言预训练在视觉表征学习方面取得了重大进展。首先通过在大规模的视觉语言对中训练，实现将图像与原始文本对齐；之后，在下游任务中，使用精心选择的文本作为提示进行零样本预测。而本文展示了一种除了提示调整之外的一种更好的实现视觉语言模型的方法，即提出*CLIP-Adapter*，通过使用视觉或语言**分支上的特征适配器**来对**模型进行微调**。

具体来说，*CLIP-Adapter* 采用额外的瓶颈层来学习新特征，并与原始预训练特征进行残差式特征混合。从而能够在保持简单设计的同时，超越上下文优化。

# 引言

例如分类、目标检测以及语义分割等视觉理解任务，都是基于良好的架构设计和大规模高质量的数据集得到了显著地提升；但是大规模高质量数据集获取是困难的，且成本较高。而为了解决这个问题，“预训练-微调”范式得到了广泛应用，即在*Imagenet* 等大规模数据集上进行预训练，然后在各种下游任务上进行微调。但是这种方式也仍然需要大量注释才能在许多下游任务上进行微调。

然后*CLIP* 被提出用于解决视觉任务，通过利用大规模噪声图像-文本对进行对比学习，从而实现将视觉类别放入合适的手工制作的提示模板中，达到不需要任何注释在各种视觉分类任务上实现了良好的效果。

尽管基于提示的零样本迁移学习表现出了良好的性能，**但设计好的提示仍然是一个工程问题**，需要大量的时间和领域知识。为了解决这个问题，上下文优化进一步提出用少量样本示例学习连续软提示，以取代精心选择的硬提示。而本文提出了另一种不同的方法*CLIP-Adapter*，使用**特征适配器**而不是***prompt* 调整**来更好的适应视觉语言模型，只对轻量级的附加特征适配器进行微调。

*CLIP-Adapter* 采用**轻量级瓶颈架构**，通过**减少参数数量**来防止小样本学习潜在的过拟合问题。*CLIP-Adapter* **仅在视觉或语言主干的最后一层**之后添加了**两个额外的线性层**。这是因为*CLIP* 原始的预训练编码器已经具备了强大的表示能力，因此只需要以**残差**的形式进行轻量级的**自适应**。*CLIP-Adapter* 通过残差连接将原始的零样本视觉或语言嵌入与相应的微调特征混合在一起。 通过这种“残差式混合”，*CLIP-Adapter* 可以同时利用原始 *CLIP* 中存储的知识和从少样本训练示例中新学到的知识。

架构图![架构图](https://pic.imgdb.cn/item/67305f7fd29ded1a8c484a5e.png)
模型对比图![模型对比图](https://pic.imgdb.cn/item/6730607dd29ded1a8c4908b0.png)
从上图中，首先需要明确的是分类任务就是根据一系列权重参数，对特征输入进行加权计算，然后得到分类结果。其中，**绿色区域部分**代表简单的用于图像分类的*pipeline*（指从原始数据输入到最终分类结果输出的一系列步骤或过程）。**红色区域部分**是原始的*CLIP* 模型，这个相较于绿色区域部分的改进就是加入了一个语言引导的权重参数（即权重参数是由文本特征决定的）。**黄色区域**是本文的另一个参考模型*CoOp*（这个也是对*CLIP* 进行了改进，但是改进方法是调整*prompt*），相较于*CLIP* 原始模型，这个模型并没有使用固定的*prompt* 模板，而是学习生成这个模板，从而达到更好的模型效果。**蓝色区域部分**即为本文提出的模型，从这个模型工作流程出可以清楚的看到，在编码器得到对应的特征输出之后，模型会使用*Adapter* 重新得到一个特征输出，最后与编码器输出进行一个残差融合。该模型在训练过程中并不会对*CLIP* 编码器进行反向传播，而是对*Adapter* 进行训练；与*CoOp* 相比（需要对*CLIP* 的*text* 编码器进行训练），**训练的参数量大大减少了**。

# 3.模型介绍

使用深度神经网络进行分类的框架是，给定一副图像*I（H×W×3)*，经过神经网络主干（由CNN、Transformer或者二者组合组成），将*I* 转换为特征流行*f(D)*（D表示特征维数）。为了进行分类将图像特征向量f与分类权重矩阵W相乘，最后得到一个K维的logit，再用Softmax将logit转换维K个类的概率向量。整个过程可以表述为下面公式：![图像分类公式](https://pic.imgdb.cn/item/67306db0d29ded1a8c5397a9.png)
而本文关注的是，使用少样本进行分类。如果直接使用少样本从头训练主干和分类器，容易导致模型在某些数据集上会发生过拟合，在测试分割中性能会严重下降。通常，少样本学习的代表性范例是首先在大规模数据集上对主干进行预训练，然后通过直接进行零样本预测或对少样本进一步微调将学到的知识转移到下游任务。

*CLIP-Adapter* 通过微调额外的特征适配器，在少样本图像分类上实现更好的视觉语言模型。之前广泛采用的“预训练-微调”范式由于参数数量巨大且训练样本短缺，在少样本设置下无法对整个 *CLIP* 主干进行微调。因此，本文提出了 *CLIP-Adapter*，它只在 *CLIP* 的语言和图像分支上附加少量额外的可学习瓶颈线性层，同时在少样本微调期间保持原始 *CLIP* 主干不变。然而，带有附加层的简单微调仍然可能在少样本示例上陷入过度拟合。为了处理过拟合问题并提高 *CLIP-Adapter* 的鲁棒性，我们进一步采用残差连接将微调后的知识与 *CLIP* 主干中的原始知识动态融合。

适配器的工作流程用公式表示如下（第一张图片是两个连接层，第二张是残差连接），根据前面的描述，我们知道该模型所谓的适配器，是在*CLIP* 编码器输出之后加了两层线性层，公式（第一个）也是符合前面描述的。第二个公式里的参数*α* 和*β* 是残差比（根据原文后面实验细节的描述，残差比可以反映预训练数据集与微调数据集之间的差异程度，差异程度越大的话，往往残差比的数值会较高），帮助调整原有知识的程度![适配器公式](https://pic.imgdb.cn/item/673071cfd29ded1a8c56e0be.png)![残差连接公式](https://pic.imgdb.cn/item/67307285d29ded1a8c577d55.png)
在训练过程，两个适配器还是按照原来的*CLIP* 对比损失进行优化。损失函数公式![损失函数公式](https://pic.imgdb.cn/item/673073dfd29ded1a8c588d62.png)

> 根据后面的实验细节描述,会发现仅使用视觉编码器比仅使用文本编码器的效果好,文章认为是预训练和微调数据集中的视觉特征之间的语义差距大于文本特征之间的语义差距。但是文章的实验结果显示,同时使用两个适配器的效果是要差于仅使用视觉适配器的效果,文章认为文本和视觉适配器可能会捕获冗余信息,甚至互相冲突.

