---
layout: post
title: 'CONTEXTUAL SELF-PACED LEARNING FOR WEAKLY SUPERVISED SPATIO-TEMPORAL VIDEO GROUNDING ICLR2025😐'
subtitle: '面向弱监督时空视频定位的上下文自定进度学习'
date: 2025-10-29
author: Sun
cover: 'https://pic1.imgdb.cn/item/69005ee93203f7be00aa5780.png'
tags: 论文阅读
---

> [CONTEXTUAL SELF-PACED LEARNING FOR WEAKLY SUPERVISED SPATIO-TEMPORAL VIDEO GROUNDING](https://arxiv.org/abs/2501.17053)

> ❌❌[github地址暂未公布代码](https://github.com/AKASH2907/copsal-weakly-stvg)
> 
> 📌作者单位
> 
> 1. University of Central Florida
> 2. Georgia Institute of Technology

# 1.文章针对痛点

这篇文章关注的是弱监督时空视频定位任务，文章主要是探索现有*SOTA* 检测模型在*WSTVG* 任务的潜力。

尽管这些模型具有鲁棒的零样本检测能力，但是直接迁移会出现一些限制，**包括不一致的时间预测、对复杂查询不充分的理解以及对不同场景适应能力的挑战**。具体来说，首先，它在时间一致性方面遇到了困难，经常跨帧切换对象焦点，缺乏对时间定位的清晰理解。其次，尽管接受了大规模图像文本数据集的训练，但它很难处理复杂或不平衡的查询，特别是当同时描述多个对象或活动时。最后，模型的性能在具有大量对象的密集场景中下降，其中准确的定位变得至关重要。

# 2.主要贡献

为了解决这些问题，文章提出了 *CoSPaL*，这是一种增强 *STVG* 空间和时间定位的新方法。 *CoSPaL* 引入了三个关键组件：(a) *Tubelet Phrase Grounding (TPG)*，它将文本查询链接到时空 *tubelet*（跨帧的边界框），从而随着时间的推移改进对象跟踪。 (b) 上下文参考定位，通过微调网络的注意力，以准确定位查询中提到的相关*tubelet*，确保在空间和时间上更精确的对象识别。 （c）自定进度场景理解，一种逐渐增加任务复杂性的训练策略，允许模型从粗略预测开始并逐步细化它们。这种结构化方法显着提高了模型在复杂场景中的适应性和鲁棒性。

原文总结贡献如下：

1. 我们提出*CoSPaL*，第一个基于基础模型解决弱监督时空视频接地问题。
2. 我们提出上下文推荐定位，它从查询中提取上下文信息并增强网络的时空接地能力。
3. 我们引入了自定进度场景理解训练方案，使网络能够应对复杂的挑战性场景。

# 3.实现流程

文章提出模型的架构图如下所示。空间定位部分与之前看过的一篇弱监督文章一样，都是先检测所有目标对象的定位框出来。

![模型架构图](https://pic1.imgdb.cn/item/690063c03203f7be00aa8ef0.png)

# 4.实现细节

* **视觉编码器**：视觉编码分别出现在架构图中的空间定位和时间定位给。空间定位给的视觉编码是直接使用*G-DINO* 并且添加了追踪器的相关部分，从而可以生成时空管道。时间定位使用的编码器文章提到的是*I3D*（在目前的时空视频定位任务上还没有看到过使用该视觉编码器的，感觉反而是时刻检索等方向使用的较多）。
* **查询编码器**：文章是写的查询编码器，但是我感觉主要还是说的文本编码器吗，这里文章提到使用的是*BETR*。
* **空间定位模块**：该模块使用多模态对比学习优化来突出单词和 *tubelet* 之间的关系。1️⃣文章的见解是，为了找到两种模式之间共享的最大互信息，首先需要将它们投影到同一空间中。在空间定位的编码器提取视觉特征之后，这些特征之间在时间上没有任何相互作用。为了在它们之间建立联系并增强时间特征，文章应用时间自注意力块来生成更新的小管特征，有助于模型突出显示提供更多上下文信息的帧。2️⃣之后，将所有目标的时空管道投影到相同的空间，按照公式1计算分数。这个公式，我的理解是计算每个目标对象与单词的分数。3️⃣在这个模块，也引入了对比学习损失，如下公式2。

$$
\mathrm{softmax}(f_{T_k},f_{w_m})=\frac{exp(\mathrm{SIM}(f_{w_m},f_{\bar{T_k}}))}{\sum_{k^{^{\prime}}=1}^{K}exp(\mathrm{SIM}(f_{w_m},f_{\bar{T_k^{^{\prime}}}}))} 	\quad(1)
$$

$$
\mathcal{L}_{s}=-\sum_{m=1}^{N}\left(\log\frac{exp(\mathbf{A}_{\mathrm{T}}(f_{T_{k}},f_{w_{m}}))}{\sum_{k^{\prime}=1,(k^{\prime}\neq k)}^{K}exp(\mathbf{A}_{\mathrm{T}}(f_{T_{k^{\prime}}},f_{w_{m}})))}\right)	\quad(2)
$$

* **时间定位模块**：这个模块的具体过程是先将视觉编码器得到的视觉特征与原始的文本查询特征进行交叉注意力；之后，对原始文本查询做掩码处理，得到全局查询和局部查询，具体可见架构图的*C* 部分，（但是这个地方我不太清楚是怎么处理全局查询和局部查询的，不太清楚是直接拼接还是怎么做的）；之后也是与第一步得到的特征进行交叉注意力。

对于文章的*CRG* 设计和自定进度学习可以参考文章原文。

# 5.模型性能


![模型性能1](https://pic1.imgdb.cn/item/690181a13203f7be00ae0b52.png)

# 6.改进/挑战/问题/想法

* **想法**：这篇文章提到的难点我认为确实是迁移图像定位方法存在的问题，首先就是时序一致性表现不好，怎么解决时序一致性是一个很好的点，这篇文章是使用重建的方法，强制视觉和文本查询之间的语义一致性，但是文章的这部分我认为没有写的很清楚。

