---
layout: post
title: 'VideoGrounding-DINO扩展论文阅读'
subtitle: 'Grounding DINO, TubeDETR, STCAT方法'
date: 2024-12-28
author: Sun
cover: ''
tags: 论文阅读
---

> VideoGrounding-DINO那篇论文的一些扩展阅读，主要是那篇文章参考模型，已经对比的两个模型

# *Grounding DINO* 方法扩展阅读

这篇文章（ECCV2024）是在图像定位任务上的开放集任务。这篇文章提出可以利用一些在大数据集上训练得到的模型，来实现分布外泛化能力。

先看一下模型架构图：![模型架构图](https://pic.imgdb.cn/item/676f4eafd0e0a243d4ebd46b.png)

文章认为相较于仅使用图像模态来实现开放集任务，加上文本模态会更容易，因为文本模态本身就具有良好的泛化能力。因此在图像定位任务中加入了文本模态，其次，文章认为要实现良好的效果，需要加强多模态之间的融合，因此在重要阶段基本都进行了双模态融合。另一个值得学习的模块，就是提出了一个文本引导的查询选择模块，这个模块的任务就是选择与文本更相关的特征作为后面解码器的查询。

# *TubeDETR* 方法扩展阅读

这篇文章（CVPR2022）其实并没有提到具体的动机是什么，也就是说针对当前任务的什么问题解决。感觉只是这个是新出来的任务，然后做一下提高一下性能。

具体来说，这篇文章是典型的基于*DETR* 模型，也就是先利用编码器提取文本和视频特征，然后再利用一个编码器融合模态，之后经过解码器解码，最后使用两个分类头分类。大体过程就是这样，然后再在中间的某些模块做一些改进。

先看一下这篇文章的整体架构图：![整体架构图](https://pic.imgdb.cn/item/676f4c9bd0e0a243d4ebd41c.png)

这篇文章是为了提升速度方面的性能。之前的方法在融合双模态特征时，都是将视频模态的每一帧都与文本特诊融合，但是文章认为这样工作太多了，也没必要。所以文章采取将视频分为一个个片段，也就是图中描述的*M Clip*。然后随机选取片段中的某一帧进行融合，这个就是编码的改进，后面解码没有什么特殊的地方。

编码的具体描述图：![编码](https://pic.imgdb.cn/item/676f4d37d0e0a243d4ebd440.png)
可以看到就是随机选取一帧进行融合，融合的具体方法就是先连接，然后经过一个*Transformer* 编码器，得到融合后的特征后；将这个特征复制*k* 份（也就是*Clip* 的长度），最后加上原来的视频特征，再经过一个函数变化，就得到最后的融合完的特征。

# *STCAT* 方法扩展阅读

就是*VideoGrounding-DINO* 那篇文章提到的两个基于闭集的监督方法，一个是*NeurIPS, 2022* 提出的*STCAT* 方法。这个方法主要解决的问题是视频文本模态对齐问题和预测不一致性问题，前者是指之前的方法都是对单帧进行判断，这样容易导致目标对象出现上下文不一致的情况；后者是指预测出现前后不一致。但是总的来说还是模态对齐没处理好。

这篇文章就提出了使用一个全局的可学习标记表示来改善多模态对齐，也就是由这个全局表示记录文本输入信息并且从全局把控模态交互。

全局架构图：![全局架构图](https://pic.imgdb.cn/item/676a6f1ad0e0a243d4e93903.png)

从架构图中可以看出：

* 前面就是简单的提取特征，文章提到的是使用*ResNet* 提取视觉特征，*BETR* 提取文本特征；之后使用一个线性投影层，将二者映射到相同的空间维度。
* 下一步就是对这两个模态特征进行编码融合。具体来说，<mark>先进行空间交互，学习局部特征</mark>，就是直接将这两个模态特征进行连接；此外，加了一个局部可学习标记表示，用于学习单帧的局部特征表示。**（注意：这个表示最后是包含视觉和文本模态特征的。）** <mark>之后进行时间交互，学习全局特征。</mark>具体来说，就是加入了一个全局的可学习标记表示，把握不会出现文本模态和视频模态对应错误问题。（例如，文本模态提到了一个穿蓝衣服的人，如果没有全局表示把握的话，可能视频中有两个都穿蓝衣服，那么在对齐过程中，很可能某些帧就会出现对齐错误，也就是对齐的这个蓝衣服对象，其实并不是我们真正关心的目标对象）
* 第三步是模板生成器。这一步是为了生成查询，便于下一步解码。这个是一种基于模板的机制来关联和限制所有视频帧的预测。具体来说，生成的模板由内容项和位置项组成。内容项是前面的全局表示，由所有帧共享，以保持查询语句所描述的相同语义；而位置项根据其外观以每帧为特征，也就是上一步图像的黄色框部分。这一步就全局表示把控是否认为某一帧是应该预测的，也就是相当于给一个正反馈。
* 最后一步是解码器。将上一步生成的查询转换为可以预测的管道，其中自注意力模块是用于解码时间维度的，交叉注意力模块用于解码空间维度。



