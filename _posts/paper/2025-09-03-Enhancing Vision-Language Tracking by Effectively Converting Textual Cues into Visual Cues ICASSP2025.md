---
layout: post
title: 'Enhancing Vision-Language Tracking by Effectively Converting Textual Cues into Visual Cues ICASSP2025😊'
subtitle: '通过有效地将文本线索转化成视觉线索来增强视觉-语言跟踪'
date: 2025-09-03
author: Sun
cover: 'https://pic1.imgdb.cn/item/68b7d5d358cb8da5c876009c.png'
tags: 论文阅读
---

> [Enhancing Vision-Language Tracking by Effectively Converting Textual Cues into Visual Cues](https://ieeexplore.ieee.org/document/10888064)

> 💐💐[仓库](https://github.com/XiaokunFeng/CTVLT)
> 
> 📌作者单位
> 
> 1. 中科大
>    这篇文章和上一篇是同一个团队的

# 1.文章针对痛点

这篇文章关注的是视觉-语言跟踪，这个任务是在第一帧给定一个文本（文本仅描述该帧目标对象的状态）和一个模板块（只是包含了目标对象，但是并没有边界框），根据给定的提示和视频，定位后面视频内容对应的目标对象。

与上篇文章关注文本在视觉变化下，可能与目标对象是不对应的问题不同。<mark>这篇文章关注的是在该任务中，文本与视觉特征的数量是不一致的，在任务中往往会具有大量的视觉特征可利用，但是文本特征一个视频就只有一个文本描述，因此文本模态和视觉模态是不平衡的</mark>；所以目前的*VLT* 模型，<mark>大多是具有良好的视觉模态处理能力，但是在文本模态处理上略显局限，很难利用文本特征来引导跟踪</mark>。


# 2.主要贡献

针对上面的问题，文章提出了一种新的文本线索利用方法*CTVLT*，该方法的核心是利用基准定位模型的强大文本-图像对齐能力，先把文本线索转化成视觉线索，从而方便模型理解。

具体而言，文章设计了一个文本提示映射模块，以将文本提示转换为高度可解释的视觉提示。根据基准定位模型编码的对齐的文本图像对，文章提出了一种免训练的类似自注意的细化机制，以生成与文本提示相对应的目标分布热图。接下来，引入了热图引导模块，该模块将生成的热图与搜索图像深度整合，最终使文本提示有效利用。文章将这种创新的文本提示利用方法无缝地集成到视觉跟踪器中，将其转换为相应的视觉跟踪器。


# 3.实现流程

文章提出模型的架构图如下所示。

![模型架构图](https://pic1.imgdb.cn/item/68b7e0bb58cb8da5c8767a68.png)

# 4.实现细节

* **编码器**：1️⃣视觉编码器。编码待处理图像和模板图像，同时将模板图像的特征嵌入在搜索图像特征中，得到$$f_x^T$$。
* **基准定位模型模块**：利用基准模型的骨干和特征增强，通过将语言描述和搜索图像传入该模块中，获得了对齐的文本提示特征$$f_l^G$$和搜索功能$$f_x^G$$。
* **文本线索投影模块**：该模块首先通过下面公式1获得了基准定位模型不同尺度的图像特征，通过分析这些特征，文章认为选择浅层的特征更容易得到细粒度特征，获取的热图会相对效果更好。最后通过公式2和3得到最终的文本转图像的热图。

$$
f_{xk}^{G}=f_{x}^{G}(S_{k},E_{k}),A_{k}=(f_{xk}^{G}\cdot(f_{l}^{G})^{T}).\mathrm{mean}(\mathrm{dim}=-1)	\quad(1)
$$

$$
f_{x1}^{sa}=f_{x1}^G\cdot(f_{x1}^G)^T\cdot f_{x1}^G	\quad(2)
$$

$$
A_1^{sa}=(f_{x1}^{sa}\cdot(f_l^G)^T).\mathrm{mean(dim}=-1)	\quad(3)
$$

* **热图引导模块**：在这个模块使用了两个卷积操作获取边界框或者说是关注的目标对象。

# 5.模型性能

![模型性能1](https://pic1.imgdb.cn/item/68b7e37758cb8da5c8768c81.png)

# 6.改进/挑战/问题/想法

* **想法**：这两篇文章的目的还是在于怎么定位图像，也就是关注的重点还是提升模型的定位跟踪能力。

