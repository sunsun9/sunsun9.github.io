---
layout: post
title: '多模态模型学习—CLIP'
date: 2024-10-20 
author: Sun
tags: 论文阅读
---

> 最近组会听了一些多模态学习分享，记录多模态模型CLIP学习。
> 
> 可学习博客[Multimodality and Large Multimodal Models (LMMs)（原版）](https://huyenchip.com/2023/10/10/multimodal.html)、[译版](https://baoyu.io/translations/lmm/multimodality-and-large-multimodal-models)。原博主的博客都挺值得看的。

多模态学习是在一个任务上利用多种不同形式的信息，例如结合视觉图像信息和文本模态的信息。CLIP是OpenAI提出的一种基于对比文本-图像对的预训练模型，就是结合了图像信息和文本信息，CLIP想解决的问题是考虑大量数据的监督的学习在现实应用中，有一定的缺陷性；因此，提出利用文本信息，可以实现对未在训练集出现过类别的样本的分类。因为监督学习都是基于大量的标记样本，但是大量的标记样本是一个很费力的工作；并且在实际应用中，不可能所有的输入都会是在训练数据集出现过的样本。

CLIP是通过构建哪个标题和哪个图像匹配的预训练，CLIP架构图：![架构图](https://pic.imgdb.cn/item/67136129d29ded1a8c4ba31a.png)

利用从互联网上检索得到的文本和图像，组成了大量的文本-图像对（原文提到大概有4亿对）。（1）每次选取batch_size个文本-图像对，先通过Image Encode（原文中提到backbone是基于ViT）和Text Encoder获取图像和文本的特征信息，之后进行对比学习预训练，让模型能够将图像特征和文本特征联系起来。（2）因为CLIP中涉及到的是每个类别的描述文本，而不是单词。因此，将单词标签都转化成了"A photo of a {object}"的形式（结果显示这种描述确实有效提高了精度），并通过编码器将句子和文本特征联系起来。（3）之后使用CLIP做零样本预测。

> 在第一个阶段提供的文本描述和图像，如下图所示：![文本-图像对](https://pic.imgdb.cn/item/67136e53d29ded1a8c63c54f.png)

> **对比学习**涉及到的主要概念就是正样本和负样本，训练的目的是更靠近正样本，并且远离负样本。在CLIP中的对比学习预训练，从图中的每一行来分析，例如第一行，I1T1这个就是正样本，而其他的对于I1来说就是负样本。原文中提到使用预测方式的学习，速度较慢；而通过使用对比学习，可以将效率提高4倍。
> 
> **零样本学习**：零样本学习是可以实现对未出现过类别的样本的分类，通过利用一些辅助信息。（1）利用属性作为辅助信息。在训练过程通过学习已知类别的样本和相关属性信息，建立文本属性特征和图像特征之间的关联。当出现一个新类别的样本时，可以图像特征查找相关的属性描述，根据所有的属性描述可以实现分类。例如，并没有见过任何鸟类的图片，但通过百科书相关描述，当出现鸟类的这张图像的时候，你可以分辨这个图像展示的是鸟类。（2）使用语义嵌入作为辅助信息。语义嵌入是将类别的描述（如文本标签、属性等）映射到高维向量空间中的过程，这些向量能够捕捉类别之间的语义关系和特征。在训练过程中，模型通过学习已知类别样本和其对应的语义嵌入之间的映射关系来进行训练。当在推理阶段出现未见过类别的样本时，模型利用这些类别的语义嵌入和输入样本的特征进行比较，找到最相似的类别嵌入，预测对应的类别标签。（3）利用对比学习作为辅助手段。在学习过程中，将类别的语义信息嵌入到特征空间中。在推理阶段，模型根据样本特征与语义嵌入之间的相似性进行推理。
> 零样本迁移是为了衡量机器学习系统的任务学习能力。

CLIP的使用个人认为还是利用了类别语义描述信息的先验知识。1）如果相关的语义描述信息并没有被收录，模型是否还能正确分类还有待考虑。2）分类结果的准确性很大程度上依赖于语义描述的准确性和全面性，如果提供的语义描述有一定的错误或者不全面，模型是否还能实现高准确度的分类。3）文中也提到了，CLIP在对专业性较强的图像进行分类时效果较差，例如卫星图像、医学图像等。

