---
layout: post
title: 'ViT模型学习'
date: 2024-11-04
author: Sun
tags: 模型算法学习
---


*ViT* 是受到了*transformer* 在*nlp* 任务上应用的启发，因此，考虑将*transformer* 架构应用于计算机视觉相关任务上，*ViT* 原文是在图像分类任务上进行了比较和实验。

构建的想法是，让*ViT* 在较大规模的数据集上进行预训练，然后在相对小规模的数据集上进行微调。

模型架构图![模型架构图](https://pic.imgdb.cn/item/6728b009d29ded1a8cc94657.png)
根据上面的架构图可以看到，（1）模型首先将输入图像分成一样大小的*patch* 块。（2）再将*patch* 块通过一个线性投影层变成*token*，并嵌入对应的位置信息。在这一步中，*ViT* 参考*BERT* 模型，也在嵌入块序列前添加了一个可学习的嵌入，在预训练和微调期间，分类头都附加到这个可学习的嵌入块中。（3）输入到*Transformer Encoder* 中，最后经过一个*MLP* 得到分类结果。