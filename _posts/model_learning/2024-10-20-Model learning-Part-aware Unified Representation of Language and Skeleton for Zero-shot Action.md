---
layout: post
title: 'Part-aware Unified Representation of Language and Skeleton for Zero-shot Action CVPR 2024'
subtitle: '用于零样本识别 基于部分感知的语言和骨骼的统一表示'
date: 2024-10-24
author: Sun
cover: 'https://pic.imgdb.cn/item/6719bce9d29ded1a8c85b4e9.png'
tags: 模型算法学习
---

> [用于零样本识别 基于部分感知的语言和骨骼的统一表示](https://openaccess.thecvf.com/CVPR2024)


# 摘要

本文认为，仅依靠对齐标签级语义和全局骨骼特征是不足以高效将局部一致的视觉知识从可见类转换到不可见类（这就是零样本识别：在可见类上完成训练，但是能很好地识别不可见类的样本）。为了能高效地完成这种零样本识别，本文引入了*Part-aware Representation between Language and Skeleton(PURLS)*，来探索在全局和局部尺度上的视觉-语义对齐。

*PURLS*为了生成跨不同级别的对齐文本和视觉表示，引入了一个新的提示块和一个新的分区块。前者使用一个预训练的*GPT-3*从原始动作标签中推理全局和局部运动的精细描述（这个感觉就是利用语言模型学习类别的精细描述）；后者应用自适应采样策略，对给定描述语义相关的所有身体关节运动的视觉特征进行分组。

# 引言

介绍了视觉输入有多种形式，例如，RGB视频、深度图像序列、点云以及骨骼序列。并说明骨骼输入形式相对其他形式的优点。

之前的研究大部分是一种全监督模式，但这种方式依赖于大量标记数据，但是要获取所有类别的一定标记数据是不现实的。由此，出现了零样本学习。本文指出，先前的零样本学习都专注训练模型，来对齐标记嵌入和视觉编码输出（来自所有骨骼特征的全局平均视觉编码）；这种方式**忽略了一些不同动作的相同表示**。本文认为，对这些共享动作的理解应该能在已见类别和未见类别之间转移，从而增强识别新动作的先验知识。另一方面，全局语义对齐专注全身动作的跨模态一致性，而不包含对这类动作的局部视觉概念的精细学习，从而限制了所学表征应用于在未见类别时的泛化能力。

为了解决上述的问题，增强模型的零样本学习能力。本文提出了PURLS，有助于在全局和局部层面进行跨模态语义对齐，以利用先验知识。**1）在语言方面**，使用大语言模型增强原始动作标签的语义。设计了一个**提示块**，使用GPT-3来生成原始动作或他们空间/时间上可分割的局部运动的详细描述。然后利用一个CLIP预训练好的文本编码器来提取文本特征。**2）在视觉方面**，引入了一个独特的**分区模块**，可以自适应地找到给定描述相关的每个关节特征的权重。最终引导模型未对齐提供一个更语义相关的视觉表征。**3）在训练阶段**，PURLS学习在全局和局部范围内投射最接近的视觉流形→文本流形，以一种平衡的方式确保所有描述的语义一致性。这样，当 PURLS 在测试期间仅将全局视觉表征映射到标签级语义进行预测时，投影层仍能提炼出部分感知知识。

# 相关工作

从三方面阐述了相关工作，分别是零样本学习、基于骨骼动作识别的零样本学习以及多模态表征学习。

1. 零样本学习：1）零样本学习训练模型依赖于来自已见类别的样本和他们所属类别的辅助信息。其训练的目的是使模型能够在新视觉特征和从未见类别获取的先验语义知识之间，建立一个可推广且有意义的联系。2）与把全局视觉特征和标记语义匹配的方法相比，基于局部表征或属性的预测能够达到更好的预测和鲁棒性能，因为后者具有更广泛的迁移能力。*PURLS*是第一篇论文实现隐藏在骨骼运动学中的局部可分解视觉概念自动知识提取和迁移。
2. 基于骨骼的动作识别上的零样本学习：目前的多种零样本学习技术都是建立在RGB输入格式上的，文中提到*SynSe-ZSL*是第一个基于骨骼的局部语义匹配的相关工作，其作者认为，动作标签通常由重复的动词和名词短语构成，**大多数动词的视觉模式可以从多个可见类别中重复学习**。因此，从标签级语义中区分动词的知识迁移可以有效提高模型对未知类别的泛化能力。本文认为，类似的直觉也可以从视觉输入方面应用，***PURLS *挖掘空间和时间局部视觉概念，并使用语言模型从原始标签推断其对齐的语义。**
3. 多模态表征学习：多模态表征学习与*ZSL*学习高度相关，在*ZSL*中，不同模态的信息转化和解释，从而进行信息交换。在本文中，提出了支持自适应知识从已见类迁移到未见类的*PURLS*，通过一个可在空间或时间上提取局部运动的强大流行对齐上。

# 本文方法

本文认为直接将已见类标记和骨骼序列的全部表征对齐的这种方法，不能捕捉到局部运动的语义信息，限制了零样本动作识别的有效性。因此，提出了PURLS。

PURLS有两个阶段。**首先**会关注每个动作标记的细微描述，从全局、空间局部和时间局部角度考虑。**之后**，使用一个独特的自适应分区模块，生成、对齐来自每个派生描述的相对应关节特征的视觉表征。

整体架构图如下：![整体架构图](https://pic.imgdb.cn/item/67174fa1d29ded1a8c071f21.png)

### 创建基于描述的文本特征

本文认为**动作**是能够在时间和空间上分解的局部身体运动的**一个具体组合**。为了提取这些底层的语义，本文使用*GPT-3*为这些不同尺度的运动生成局部和全局的文本描述，从而丰富原始标签。
示例如下，*Table1*是局部空间描述，*Table2*是局部时间描述和全局描述：![文本描述](https://pic.imgdb.cn/item/67175185d29ded1a8c08d481.png)
得到这些描述后，使用预训练好的***CLIP*的*text Encoder***模型获取文本嵌入*F*。注：这些描述需要转换成*CLIP*支持的描述格式。

### 分区骨骼特征表示

按照*Syntactically guided generative embeddings for zero-shot skeleton action recognition*论文的方法，获取标准输入I；然后使用预训练的*Shift-GCN*提取视觉特征得到*G*。在这个转化过程中，本文为了简便计算，平均了出现的表演者的特征。

为了从视觉特征*G*中提取局部空间和局部时间表征，本文使用了一个分区模块。一个直接生成局部空间表征的方法是，手动将骨骼节点分成若干部分；同时时间分区采取沿着时间维度将*G*平均分成*Z*个连续块。本文将这种方法称为*static partitioning*，尽管这种方法简单，但是在提取对应描述的视觉信息时，经常显现不稳定性；并且需要做大量的实验来找寻最适合的划分方式。

因此，本文认为更灵活的生成局部表征的方式是**自适应**的从*G*中采样所有与描述相关的特征。引入了基于交叉注意力的自适应分区模块，该模块架构图如下![自适应架构图](https://pic.imgdb.cn/item/671757b6d29ded1a8c12e0fa.png)
其中，*Q*和*K*是*F*和*G*分别进行一个而线性变换得到的；*Q*和*K*进行叉积，再经过归一化和*softmax*处理得到注意力矩阵*A*（*Aij*表示第*i*个描述和第*j*个节点的相关性）；最后*G*经过所有节点特征的加权和得到*R*，权重由*A*定义。注：因为*A*里面已经包含了各种分区信息（即局部时间、局部空间等）了，因此得到的*R*不用经过其他处理，就自然是分区的。

### 对齐双模态表征

为了实现部分感知匹配，本文设计了一个*MLP*模块，将视觉表征映射到描述编码的对应分布中，即将每一个*Ri*投影到文本嵌入空间Vi中。最后在*V*和*F*之间构建了一个对比学习，公式如下（和*CLIP*的对比学习有点像）![对比学习公式](https://pic.imgdb.cn/item/67175c6bd29ded1a8c1c02a4.png)
模型总损失公式：![模型总损失公式](https://pic.imgdb.cn/item/67175d58d29ded1a8c1d2543.png)
测试阶段，即在所有的类别中找到一个标记，使得对齐损失最小。公式：![测试](https://https://pic.imgdb.cn/item/67175da8d29ded1a8c1d6a9c.png)

# 实验

本文在三个数据集上进行了实验和之后的各种评估实验（其他数据集上的实验也有，论文没有详细介绍），分别是*NTU-RGB+D 60* 、*NTU-RGB+D 120*和*Kinetics-skeleton 200*。其中第三个数据集是选取了*Kinetics-skeleton 400*的前200个类别，因为在演示过程中发现，随着未见类别的增加，任务难度超过了模型所能提取特征的能力，而这不是本文要探究的问题。

### 消融实验

本文进行了四个方面的消融实验。

* **通用性**：探寻了使用不同的骨骼编码器和描述生成器，并与前文提到的直接将语义特征和全局对齐的这种方式进行了比较。实验结果如下，虽然PoseC3D效果较好，但是该模型是将2D热图转换成了卷积特征图，不利于后续的静态分区。![通用性](https://pic.imgdb.cn/item/6719b67ad29ded1a8c7ef4e2.png)
* **标记语义和描述语义**：大部分情况下，描述语义带来的效果优于标记语义。
* **分区阶段**：对比了静态分区和自适应分区。![标记语义vs描述语义 & 分区阶段 消融实验](https://pic.imgdb.cn/item/6719b83dd29ded1a8c80b68e.png)
* **局部语义和聚合**：根据消融实验，会发现在结合局部损失（包括时间和空间损失）的时候效果会更好。![局部语义聚合](https://pic.imgdb.cn/item/6719b887d29ded1a8c8174ce.png)

> 这种文本和视觉多模态结合的方法，感觉主要是需要考虑文本怎么指导视觉或者说文本怎么起到一个辅助作用。本文的方法和CLIP的思想是一样的，都是基于标记生成文本辅助，就是说明在训练的时候还是以一种监督的方式进行的，感觉可以考虑怎么在半监督或者无监督的方式下进行。大概就是利用文本模型比如生成图像的描述，再反过来指导图像。
> 
> 另外感觉如果需要好的实验结果，其实还是需要对分析更细致一点，比如说从多方面考虑。

